{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <h1> INF393 - Máquinas de Aprendizaje  </h1>\n",
    "    <h2> Tarea 3 </h2>\n",
    "    <h3> Universidad Técnica Federico Santa Maria </h3>\n",
    "    \n",
    "</center>\n",
    "\n",
    "_Diciembre 2017_\n",
    "<p>Profesor: R. Ñanculef</p>\n",
    " <p>Ayudante: Francisco Mena</p>\n",
    " <p>Integrantes: \n",
    " <br>Alfredo Silva,\n",
    " 201373511-8</br>\n",
    " <br>Fernando Llorens, 201373528-2</br>\n",
    " \n",
    "\n",
    " \n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>1. Small Circle inside Large Circle</h1>\n",
    "<p>\n",
    "El objetivo de esta sección es experimentar con algunos modelos no-lineales sobre un problema de juguete\n",
    "generado para visualizar algoritmos de clustering. Se trata de un problema de clasificación a todas luces\n",
    "linealmente inseparable, en el sentido que, si denotamos por $x∈R^{2}$ un patrón de entrada y por y ∈{−1, 1}\n",
    "su correspondiente etiqueta, no existen $w∈R^{2}$, $b∈R$ tal que y (w$^{T}$ x + b) ≥ ρ > 0. El problema nos permite\n",
    "hacer un recorrido rápido por las grandes ideas en la búsqueda de la no-linealidad.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>(a) Escriba una función que genere (aleatoriamente) n datos etiquetados de la forma $\\{(x_1, y_1), . . . ,(x_n, y_n)\\}$,\n",
    "$x_{i}∈R^{2}$, $y_{i}∈\\{0, 1\\}$, con una distribución de probabilidad que refleje la configuración linealmente\n",
    "inseparable que muestra la Fig. 1 (En el PDF original)\n",
    ". Utilice esta función para crear 1000 datos de entrenamiento y 1000 datos de pruebas. Para medir la tendencia de los modelos a sobre-ajuste, agregue un 5 % de ruido al dataset, generando X’s cercanos a la frontera. Genere un gráfico que muestre datos de entrenamiento y pruebas, identificando cada clase con un color diferente.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.utils import check_random_state\n",
    "from sklearn.model_selection import train_test_split\n",
    "def do_circles(n=2000,noisy_n=0.05):\n",
    "    generator = check_random_state(10)\n",
    "    linspace = np.linspace(0, 2 * np.pi, n // 2 + 1)[:-1]\n",
    "    outer_circ_x = np.cos(linspace)\n",
    "    outer_circ_y = np.sin(linspace)\n",
    "    inner_circ_x = outer_circ_x * .3\n",
    "    inner_circ_y = outer_circ_y * .3\n",
    "    X = np.vstack((np.append(outer_circ_x, inner_circ_x),\n",
    "                        np.append(outer_circ_y, inner_circ_y))).T\n",
    "    y = np.hstack([np.zeros(n // 2, dtype=np.intp),\n",
    "                                np.ones(n // 2, dtype=np.intp)])\n",
    "    X += generator.normal(scale=noisy_n, size=X.shape)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "    test_size=0.5, random_state=42)\n",
    "    return X_train,y_train,X_test,y_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "Para lo que sigue de la actividad utilice la siguiente función para graficar las fronteras de clasificación en\n",
    "base a la probabilidad, definida por un algoritmo, de un ejemplo a pertenecer a una clase en particular.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_classifier(clf,X_train,Y_train,X_test,Y_test,model_type):\n",
    "    f, axis = plt.subplots(1, 1, sharex='col', sharey='row',figsize=(12, 8))\n",
    "    axis.scatter(X_train[:,0],X_train[:,1],s=30,c=Y_train,zorder=10,cmap='cool')\n",
    "    axis.scatter(X_test[:,0],X_test[:,1],s=20,c=Y_test,zorder=10,cmap='Greys')\n",
    "    XX, YY = np.mgrid[-2:2:200j, -2:2:200j]\n",
    "    if model_type == 'tree':\n",
    "        Z = clf.predict_proba(np.c_[XX.ravel(), YY.ravel()])[:,0]\n",
    "    elif model_type == 'ann':\n",
    "        Z = clf.predict(np.c_[XX.ravel(), YY.ravel()])\n",
    "    else: raise ValueError('model type not supported')\n",
    "    Z = Z.reshape(XX.shape)\n",
    "    Zplot = Z >= 0.5\n",
    "    axis.pcolormesh(XX, YY, Zplot ,cmap='YlGn')\n",
    "    axis.contour(XX, YY, Z, alpha=1, colors=[\"k\", \"k\", \"k\"], linestyles=[\"--\", \"-\", \"--\"],\n",
    "    levels=[-2, 0, 2])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "(b) Demuestre experimentalmente que una red neuronal artificial correspondiente a 1 sola neurona (i.e. sin capas escondidas) no puede resolver satisfactoriamente el problema. Puede utilizar la función de activación y el método de entrenamiento que prefiera. Sea convincente: por ejemplo, intente modificar\n",
    "los parámetros de la máquina de aprendizaje, reportando métricas que permitan evaluar el desempeño\n",
    "del modelo en el problema con cada cambio efectuado. Adapte también la función plot classifier para\n",
    "que represente gráficamente la solución encontrada por la red neuronal. Describa y explique lo que\n",
    "observa, reportando gráficos de la solución sólo para algunos casos representativos.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD\n",
    "n_h=1\n",
    "model = Sequential()\n",
    "model.add(Dense(1, input_dim=X_train.shape[1], kernel_initializer='uniform', activation='relu'))\n",
    "model.add(Dense(n_h, init='uniform', activation='sigmoid'))\n",
    "model.compile(optimizer=SGD(lr=1), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train, Y_train, epochs=50, batch_size=100, verbose=1)\n",
    "scores = model.evaluate(X_test, Y_test)\n",
    "test_acc = scores[1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "(c) Demuestre experimentalmente que una red neuronal artificial con 1 capa escondida puede resolver satisfactoriamente\n",
    "el problema obtenido en (a). Puede utilizar la arquitectura y el método de entrenamiento\n",
    "que prefiera, pero en esta actividad puede optar tranquilamente por usar los hiper-parámetros que se\n",
    "entregan como referencia en el código de ejemplo. Cambie el número de neuronas $N_h$ en la red entre 2 y 32 en potencias de 2, graficando el error de entrenamiento y pruebas como función de $N_h$. Describa y explique lo que observa. Utilice la función plot classifier, diseñada anteriormente, para construir gráficos\n",
    "de la solución en algunos casos representativos.\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#...\n",
    "n_h=32\n",
    "model = Sequential()\n",
    "model.add(Dense(n_h, input_dim=X_train.shape[1], kernel_initializer='uniform', activation='relu'))\n",
    "model.add(Dense(1, kernel_initializer='uniform', activation='sigmoid'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "(d) Demuestre experimentalmente que stump (árbol de clasificación de 1 nivel) no puede resolver satisfactoriamente\n",
    "el problema anterior. Puede utilizar el criterio y la función de partición que prefiera. Sea\n",
    "éconvincente: por ejemplo, intente modificar los parámetros de la máquina, reportando metricas que\n",
    "permitan evaluar el desempeño del modelo en el problema con cada cambio efectuado. Adapte también\n",
    "la función plot classifier para que represente gráficamente la solución encontrada por el árbol. Describa\n",
    "y explique lo que observa, reportando gráficos de la solución sólo para algunos casos representativos.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier as Tree\n",
    "clf=Tree(criterion='gini',splitter='best',random_state=0,max_depth=1)\n",
    "clf.fit(X_train,Y_train)\n",
    "acc_test = clf.score(X_test,Y_test)\n",
    "print \"Test Accuracy = %f\"%acc_test\n",
    "print clf.tree_.max_depth\n",
    "plot_classifier(clf,X_train,Y_train,X_test,Y_test,'tree')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "(e) Demuestre experimentalmente que un árbol de clasificación de múltiples niveles puede resolver satisfactoriamente\n",
    "el problema estudiado. Puede utilizar el criterio y la función de partición que prefiera,\n",
    "pero puede optar tranquilamente por usar los hiper-parámetros que se entregan como referencia en el\n",
    "código de ejemplo. Cambie el número de niveles admitidos en el árbol Nt entre 2 y 20, graficando el\n",
    "error de entrenamiento y pruebas como función de Nt. Describa y explique lo que observa. Utilice la\n",
    "función plot classifier, diseñada anteriormente, para construir gráficos de la solución en algunos casos\n",
    "representativos.\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#...\n",
    "n_t=8\n",
    "clf=Tree(criterion='gini',splitter='best',random_state=0,max_depth=n_t)\n",
    "clf.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "(f) Como ya se demostró experimentalmente que este problema es linealmente inseperable, ahora se pide\n",
    "experimentar otra alternativa. Para ello deberá realizar una proyección de los datos a un nuevo espacio\n",
    "dimensional (manifold) en el cual se reconozcan sus patrones no lineales, para poder trabajarlos con\n",
    "fronteras lineales. Utilice la técnica de PCA con la ayuda de un Kernel Gaussiano para extraer\n",
    "sus vectores con dimensión infinita de mayor varianza.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kpca = KernelPCA(n_components=2,kernel=\"rbf\", gamma=5)\n",
    "kpca = kpca.fit(X_train)\n",
    "Xkpca_train = kpca.transform(X_train)\n",
    "Xkpca_test = kpca.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>(g) Ajuste un algoritmo de aprendizaje con fronteras lineal para los datos proyectados en este nuevo espacio\n",
    "que captura sus componentes no lineales, muestre graficamente que el problema ahora puede ser resulto\n",
    "con estos métodos. Reporte métricas para evaluar el desempeño, comente y concluya.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>2. Bike Sharing: Predicción de Demanda Horaria</h1>\n",
    "<p>\n",
    "En esta sección simularemos nuestra participación en el desafío Bike Sharing Demand de Kaggle. El\n",
    "objetivo es predecir la demanda de bicicletas sobre la red Capital Bikeshare de la ciudad de Washington,\n",
    "D.C., en función de la hora del día y otras variables descritas en la tabla 1. En principio, y como muestra\n",
    "la figura, la función es altamente no lineal y no determinista como función de la hora del día. Su objetivo\n",
    "será entrenar un modelo para obtener un puntaje correspondiente al top-100 del “leaderboard” final, es\n",
    "decir superior o igual a 0.37748.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "(a) Cargue los datos de entrenamiento y pruebas como dataframes de pandas. Describa las variables involucradas\n",
    "en el problema, explorando el tipo de datos de que se trata, el número de valores distintos y, si\n",
    "corresponde, un gráfico (e.g. un histograma) que resuma su comportamiento. Su primera operación de\n",
    "pre-procesamiento de datos será obtener la hora del día desde el campo fecha (que en este momento es\n",
    "de tipo string), creando una nueva columna denominada hour y de tipo int. Para hacer esta operación\n",
    "se concatenarán los dataframes de entrenamiento y pruebas y luego se volverán a separar manteniendo\n",
    "la separación original.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "dftrain = pd.read_csv('bike_sharing_train.csv')\n",
    "dfval = pd.read_csv('bike_sharing_val.csv')\n",
    "dftest = pd.read_csv('bike_sharing_test.csv')\n",
    "ntrain = len(dftrain)\n",
    "nval = len(dftrain) + len(dfval)\n",
    "df = pd.concat([dftrain,dfval,dftest])\n",
    "print '\\nSummary - dataframe completo:\\n'\n",
    "print df.describe()\n",
    "df['hour'] = pd.to_datetime(df['datetime']).apply(lambda x: x.strftime('%H'))\n",
    "df['hour'] = pd.to_numeric(df['hour'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "(b) Entrene un árbol de regresión para resolver el problema usando parámetros por defecto. Con este\n",
    "fin, construya una matriz $X_{train}$ de forma $n_{train} x$ $d1$ que contenga los datos de entrenamiento en sus\n",
    "filas, seleccionando las columnas que desee/pueda utilizar para el entrenamiento. Implemente además, la\n",
    "función de evaluación que hemos definido anteriormente para este problema. Evalúe el árbol de regresión\n",
    "ajustado a los datos de entrenamiento sobre el conjunto de entrenamiento y pruebas. Construya un\n",
    "gráfico que compare las predicciones con los valores reales. En este punto usted debiese tener un modelo\n",
    "con puntaje del orden de 0.59, lo que lo dejará más o menos en la posición 2140 de la competencia.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor as Tree\n",
    "import matplotlib.pyplot as plt\n",
    "def eval_bikemodel(y_predict,y_true):\n",
    "    diff = np.log(y_predict+1.0) - np.log(y_true+1.0)\n",
    "    return np.sqrt(np.sum(np.square(diff))/len(y_predict))\n",
    "Xdf=df.ix[:,['season','holiday','workingday',\n",
    "                'weather','temp','atemp',\n",
    "                'humidity','windspeed','hour']]\n",
    "Ydf=df.ix[:,'count']\n",
    "X_train = Xdf[0:ntrain].values\n",
    "X_val = Xdf[ntrain:nval].values\n",
    "X_test = Xdf[nval:].values\n",
    "Y_train = Ydf[0:ntrain].values\n",
    "Y_val = Ydf[ntrain:nval].values\n",
    "Y_test = Ydf[nval:].values\n",
    "\n",
    "model = Tree(random_state=0)\n",
    "model.fit(X_train,Y_train)\n",
    "score_test = model.score(X_test,Y_test)\n",
    "print \"SCORE TEST=%f\"%score_test\n",
    "\n",
    "Y_pred_train = model.predict(X_train)\n",
    "Y_pred_val = model.predict(X_val)\n",
    "Y_pred_test = model.predict(X_test)\n",
    "kagg_train = eval_bikemodel(Y_pred_train,Y_train)\n",
    "kagg_val = eval_bikemodel(Y_pred_val,Y_val)\n",
    "kagg_test = eval_bikemodel(Y_pred_test,Y_test)\n",
    "print \"KAGG EVAL TRAIN =%f\"%kagg_train\n",
    "print \"KAGG EVAL TEST =%f\"%kagg_test\n",
    "plt.plot(Y_test,Y_pred_test,'.')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "(c) Mejore el árbol de regresión definido en el punto anterior haciendo modificaciones a los hiper-parámetros\n",
    "del modelo. Por ejemplo, como estos modelos tienden a sobre-ajustar, podría intentar limitar la\n",
    "profundidad del árbol (¿Por qué esto debiese ayudar?). Naturalmente, está absolutamente prohibido\n",
    "tomar este tipo de decisiones en función del resultado de pruebas. Debe realizar estas elecciones evaluando\n",
    "sobre el conjunto de validación. Si no desea utilizarlo, y prefiere implementar validación cruzada\n",
    "u otra técnica automática, tiene la ventaja de poder usar el conjunto de validación como parte del\n",
    "entrenamiento. Con estas modificaciones debiese poder mejorar su ranking en unas 300 posiciones.\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Tree(random_state=0,max_depth=20)\n",
    "model.fit(X_train,Y_train)\n",
    "Y_pred_val = model.predict(X_val)\n",
    "kagg_val = eval_bikemodel(Y_pred_val,Y_val)\n",
    "print \"KAGG EVAL VAL =%f\"%kagg_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "(d) Mejore el árbol de regresión definido en el punto anterior haciendo modificaciones sobre la representación\n",
    "utilizada para aprender desde los datos. Por ejemplo, los histogramas que construyó en el punto\n",
    "(a) así como la forma especial de la función de evaluación, sugieren una cierta transformación de la\n",
    "variable respuesta. Podría intentar también normalizando los datos o normalizando la respuesta. Otra\n",
    "opción es intentar rescatar algo más acerca de la fecha (anteriormente sólo se extrajo la hora), como por\n",
    "ejemplo el año o el día de la semana (’lunes’,’martes’, etc) que corresponde. Sea creativo, este paso le\n",
    "debiese reportar un salto de calidad muy significativo. Una observación importante es que si hace una\n",
    "transformación a la variable respuesta (por ejemplo raíz cuadrada), debe invertir esta transformación\n",
    "antes de evaluar el desempeño con eval bikemodel (por ejemplo, elevar al cuadrado si tomó raíz cuadrada).\n",
    "Con modificaciones de este tipo, podría mejorar su ranking en unas 1000 posiciones, entrando\n",
    "ya al top-1000 con un score del orden de 0.45.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['cday'] = pd.to_datetime(df['datetime']).dt.dayofweek#0:lunes,6:domingo\n",
    "df['cday'] = pd.to_numeric(df['cday'])\n",
    "Xdf=df.ix[:,['season','holiday','workingday','weather','temp','atemp',\n",
    "                                'humidity','windspeed','hour','cday']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "(e) Entrene una SVM no lineal para resolver el problema midiendo el efecto de las distintas representaciones\n",
    "que haya descubierto hasta este punto. Un detalle importante es que antes de entrenar la SVM sería\n",
    "aconsejable hacer dos tipos de pre-procesamiento adicional de los datos: (i) codificar las variables\n",
    "categóricas en un modo apropiado - por ejemplo como vector binario con un 1 en la posición del\n",
    "valor adoptado-, (ii) escalar los atributos de modo que queden centrados y con rangos comparables.\n",
    "Usando par´ametros por defecto para la SVM debiese obtener un score del orden de 0.344, quedando\n",
    "definitivamente en el top-10 de la competencia.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load dataframes as before ...\n",
    "df = pd.concat([dftrain,dfval,dftest])\n",
    "df['hour'] = pd.to_datetime(df['datetime']).apply(lambda x: x.strftime('%H'))\n",
    "df['cday'] = pd.to_datetime(df['datetime']).dt.dayofweek\n",
    "df['hour'] = pd.to_numeric(df['hour'])\n",
    "df['cday'] = pd.to_numeric(df['cday'])\n",
    "Xdf=df.ix[:,['season','holiday','workingday','weather','temp','atemp',\n",
    "                                'humidity','windspeed','hour','cday']]\n",
    "#PASO IMPORTANTE MAS ABAJO ...\n",
    "Xdf = pd.get_dummies(Xdf,columns=['season', 'weather','hour','cday'])\n",
    "Ydf=df.ix[:,'count']\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scalerX = StandardScaler()\n",
    "X_train = scalerX.fit_transform(X_train)\n",
    "X_val = scalerX.fit_transform(X_val)\n",
    "X_test = scalerX.transform(X_test)\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "model = SVR()\n",
    "model.fit(X_train,Y_train)\n",
    "Y_pred_train = model.predict(X_train)\n",
    "Y_pred_val = model.predict(X_val)\n",
    "Y_pred_test = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "(f) Mejore la SVM definida en el punto anterior haciendo modificaciones a los hiper-parámetros de la\n",
    "máquina. Naturalmente, está absolutamente prohibido tomar este\n",
    "tipo de decisiones de diseño mirando el resultado de pruebas. Debe realizar estas elecciones evaluando\n",
    "sobre el conjunto de validación. Si no desea utilizarlo, y prefiere implementar validación cruzada\n",
    "u otra técnica automática, tiene la ventaja de poder usar el conjunto de validación como parte del\n",
    "entrenamiento.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = SVR(C=1,epsilon=0.01)\n",
    "kagg_train = eval_bikemodel(Y_pred_train,Y_train)\n",
    "kagg_val = eval_bikemodel(Y_pred_val,Y_val)\n",
    "print \"KAGG EVAL TRAIN =%f\"%kagg_train\n",
    "print \"KAGG EVAL VAL =%f\"%kagg_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>(g) Evalúe el efecto de utilizar el dataset de validación para entrenamiento y seleccionar los parámetros\n",
    "estructurales del árbol de clasificación y la SVM usando validación cruzada. El código de ejemplo para\n",
    "esto ha sido proporcionado en las tareas 1 y 2, pero se adjunta de nuevo a continuación\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=10)\n",
    "mse_cv = 0\n",
    "for train, val in kf.split(Xm):\n",
    "    model = #define your model\n",
    "    model.fit(Xm[train], ym[train])\n",
    "    yhat_val = model.predict(Xm[val])\n",
    "    ytrue_val = ym[val]\n",
    "    score_fold = eval_bikemodel(yhat_val,ytrue_val)\n",
    "    mse_cv += score_fold\n",
    "mse_cv = mse_cv / 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>(h) Evalúe el efecto de utilizar un ensamblado de 2 máquinas de aprendizaje para predecir la demanda\n",
    "total de bicicletas. Un modelo se especializará en la predicción de la demanda de bicicletas de parte\n",
    "de usuarios registrados y otra en la predicción de la demanda de usuarios casuales. Hay razones claras\n",
    "para pensar que los patrones son distintos.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Ydf=df.ix[:,'count'] #demanda total\n",
    "Ydf=df.ix[:,'registered'] #demanda registrada\n",
    "Ydf=df.ix[:,'casual'] #demanda casual\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>(i) Evalúe el efecto de utilizar un algoritmo genérico para ensamblar máquinas de aprendizaje para predecir\n",
    "la demanda total de bicicletas. Puede experimentar con una sola técnica (e.g. Random Forest), discuta\n",
    "la evolución a medida que aumenta el número de máquinas.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "model = RandomForestRegressor(n_estimators=10,max_depth=max_depth,random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>3. Calidad de un vino</h1>\n",
    "<p>\n",
    "Dentro de las muchas variedades de vino existentes, algunas gustan más que otras, esto es debido al gusto\n",
    "de una persona en particular o bien a la gran cantidad de químicos y procesos que se aplican a la producción\n",
    "de vino. Para el área de negocios, el estimar cuál es la calidad de un vino en base a la apreciación del público\n",
    "es una tarea bastante difícil.\n",
    "</p>\n",
    "<p>\n",
    "Para esta actividad se trabajará con dos datasets asociados a las variantes tinto y blanco del vino portugués\n",
    "”Vinho Verde”. Debido a temas privados solo se cuenta con las caracterísstcas fisioquímicas asociadas a un\n",
    "vino en particular, los cuales corresponden a 11 atributos numéricos descritos en el siguiente link.\n",
    "Este problema puede ser abordado como clasificación de 11 clases o de regresión, ya que el atributo a estimar,\n",
    "quality, es un valor entero entre 0 y 10.\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "(a) Carge los dos dataset en un único dataframe de pandas, además de agregar una columna indicando si\n",
    "es vino tinto o blanco. Describa el dataset a trabajar.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_red = pd.read_csv(\"winequality-red.csv\",sep=\";\")\n",
    "df_white = pd.read_csv(\"winequality-white.csv\",sep=\";\")\n",
    "df = pd.concat([df_red,df_white], axis=0)\n",
    "#genere atributo 'tipo'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>(b) Aborde este problema como si fuera de clasificación binaria para predecir si un vino es de buena calidad\n",
    "o no, es decir, utilice las distintas características fisioquímicas presentes en los datos para estimar esta\n",
    "etiqueta. Para esto cree las matrices de entrenamiento y de pruebas, además de la etiqueta para ambos\n",
    "conjuntos, considerando como quality mayor a 5 un vino de buena calidad. El conjunto de pruebas\n",
    "(25 %) será utilizado únicamente para verificar la calidad de los algoritmos a entrenar.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['good_quality'] = [1 if q>5 else 0 for q in df.quality] #then remove 'quality' from df\n",
    "#train and test split over df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "(c) Entrene un solo Árbol de Clasificación de múltiples niveles para resolver el problema. Puede variar los \n",
    "hiper-parámetros que prefiera, recuerde que las decisiones no pueden ser basadas mirando el conjunto\n",
    "de pruebas. Debido al desbalanceo que se produce en las dos clases mida la métrica F1-score sobre\n",
    "el conjunto de entrenamiento y de pruebas.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "(d) Entrene un ensamblador de árboles de múltiples niveles, mediante la técnica de Random Forest. Varíe la\n",
    "cantidad de árboles de decisión utilizados en el ensamblado (n estimators), realice un gráfico resumen\n",
    "del F1-score de entrenamiento y de pruebas en función de este hiper-parámetro.\n",
    "<p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier(n_estimators=, max_depth=,n_jobs=-1)\n",
    "from sklearn.metrics import f1_score\n",
    "f1_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "(e) Entrene un ensamblador de árboles de múltiples niveles, mediante la técnica de AdaBoost. Varíe la\n",
    "cantidad de árboles de decisión utilizados en el ensamblado (n estimators), realice un gráfico resumen\n",
    "del F1-score de entrenamiento y de pruebas en función de este hiper-parámetro. Compare y analice con\n",
    "la técnica utilizada en d).\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "model = AdaBoostClassifier(base_estimator=Tree(max_depth=), n_estimators=)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "(f) Entrene alguna otra máquina de aprendizaje, elegida por usted, para resolver este problema. Elija los\n",
    "hiper-par´ametros que estime convenientes intentando aumentar el F1-score obtenido por los algoritmos\n",
    "anteriores. Compare y analice estas 4 maneras de resolver el problema definido en b).\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "(g) Defina un criterio para estimar la importancia de los distintos atributos en el ensamblado de Random\n",
    "Forest, implementelo sobre alguno de los ensambladores entrenados en d), haga un ranking de\n",
    "importancia de atributos ¿Es posible implementar este criterio sobre una técnica de boost como lo es\n",
    "AdaBoost?\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>4. Reconocimiento de Imágenes Sign Gestures</h1>\n",
    "<p>\n",
    "MNIST es un dataset muy popular de dígitos escrito a mano que a servido para probar distintos algoritmos\n",
    "de Machine Learning relacionados con Computer Vision. Buscando nuevos desafíos, investigadores generaron\n",
    "un dataset que podría usarse eventualmente en aplicaciones reales, Sign Gestures, consta de imagenes del\n",
    "lenguaje de señas, estas tienen una resolución de 28x28 pixeles representados en una escala de grises 0-255.\n",
    "La versión utilizada se atribuye a y viene separada en 27455 ejemplos de entrenamiento y 7172 casos de\n",
    "pruebas. Las clases son mutualmente excluyentes y corresponden a las letras del alfabeto (imagen en PDF original).\n",
    "</p>\n",
    "<p>\n",
    "(a) Construya una función que cargue todos los datos de entrenamiento y pruebas del problema generando\n",
    "como salida: (i) dos matrices $X_{tr}$, $Y_{tr}$, correspondientes a las imágenes y etiquetas de entrenamiento,\n",
    "(ii) dos matrices $X_t$, $Y_t$, correspondientes a las imágenes y etiquetas de pruebas, y finalmente (iii) dos\n",
    "matrices $X_v$, $Y_v$, correspondientes a imágenes y etiquetas que se usarán como conjunto de validación, es\n",
    "decir para tomar decisiones de diseño acerca del modelo. Este último conjunto debe ser extraído desde\n",
    "el conjunto de entrenamiento original y no debe superar las 7000 imágenes.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN+VALIDACION-----------------------------\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 27455 entries, 0 to 27454\n",
      "Columns: 785 entries, label to pixel784\n",
      "dtypes: int64(785)\n",
      "memory usage: 164.4 MB\n",
      "TEST-----------------------------------------\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7172 entries, 0 to 7171\n",
      "Columns: 785 entries, label to pixel784\n",
      "dtypes: int64(785)\n",
      "memory usage: 43.0 MB\n"
     ]
    }
   ],
   "source": [
    "#Si validacion no puede superar 7000 imagenes entonces no puede ser mas del 25% del entrenamiento,\n",
    "#para asegurarnos usaremos el 20\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "def load_data():\n",
    "    train = pd.read_csv('sign_mnist_train.csv')\n",
    "    test = pd.read_csv('sign_mnist_test.csv')\n",
    "    print(\"TRAIN+VALIDACION-----------------------------\")\n",
    "    train.shape\n",
    "    train.info()\n",
    "    train.describe()\n",
    "    print(\"TEST-----------------------------------------\")\n",
    "    test.shape\n",
    "    test.info()\n",
    "    test.describe()\n",
    "    y_tr = train['label']\n",
    "    x_tr = train.iloc[:,1:]\n",
    "    x_train, x_v, y_train, y_v = train_test_split(x_tr, y_tr, test_size=0.20, random_state=42)\n",
    "    y_t = test['label']\n",
    "    x_t = test.iloc[:,1:]\n",
    "    #you need to add Xval: x_v,y_v\n",
    "    return(x_train,x_v,x_t,y_train,y_v,y_t)\n",
    "x_tr, x_v, x_t, y_tr, y_v , y_t = load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>pixel9</th>\n",
       "      <th>pixel10</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "      <th>pixel784</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22706</th>\n",
       "      <td>97</td>\n",
       "      <td>65</td>\n",
       "      <td>128</td>\n",
       "      <td>44</td>\n",
       "      <td>17</td>\n",
       "      <td>67</td>\n",
       "      <td>64</td>\n",
       "      <td>66</td>\n",
       "      <td>69</td>\n",
       "      <td>72</td>\n",
       "      <td>...</td>\n",
       "      <td>189</td>\n",
       "      <td>180</td>\n",
       "      <td>134</td>\n",
       "      <td>97</td>\n",
       "      <td>76</td>\n",
       "      <td>86</td>\n",
       "      <td>94</td>\n",
       "      <td>90</td>\n",
       "      <td>91</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1231</th>\n",
       "      <td>215</td>\n",
       "      <td>216</td>\n",
       "      <td>217</td>\n",
       "      <td>223</td>\n",
       "      <td>206</td>\n",
       "      <td>176</td>\n",
       "      <td>176</td>\n",
       "      <td>191</td>\n",
       "      <td>106</td>\n",
       "      <td>122</td>\n",
       "      <td>...</td>\n",
       "      <td>133</td>\n",
       "      <td>133</td>\n",
       "      <td>132</td>\n",
       "      <td>132</td>\n",
       "      <td>134</td>\n",
       "      <td>124</td>\n",
       "      <td>167</td>\n",
       "      <td>124</td>\n",
       "      <td>23</td>\n",
       "      <td>140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>531</th>\n",
       "      <td>144</td>\n",
       "      <td>145</td>\n",
       "      <td>146</td>\n",
       "      <td>147</td>\n",
       "      <td>150</td>\n",
       "      <td>150</td>\n",
       "      <td>151</td>\n",
       "      <td>151</td>\n",
       "      <td>151</td>\n",
       "      <td>150</td>\n",
       "      <td>...</td>\n",
       "      <td>78</td>\n",
       "      <td>107</td>\n",
       "      <td>183</td>\n",
       "      <td>187</td>\n",
       "      <td>171</td>\n",
       "      <td>160</td>\n",
       "      <td>131</td>\n",
       "      <td>134</td>\n",
       "      <td>101</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21099</th>\n",
       "      <td>64</td>\n",
       "      <td>73</td>\n",
       "      <td>83</td>\n",
       "      <td>96</td>\n",
       "      <td>101</td>\n",
       "      <td>108</td>\n",
       "      <td>111</td>\n",
       "      <td>115</td>\n",
       "      <td>118</td>\n",
       "      <td>122</td>\n",
       "      <td>...</td>\n",
       "      <td>163</td>\n",
       "      <td>167</td>\n",
       "      <td>169</td>\n",
       "      <td>169</td>\n",
       "      <td>170</td>\n",
       "      <td>173</td>\n",
       "      <td>173</td>\n",
       "      <td>175</td>\n",
       "      <td>175</td>\n",
       "      <td>176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17752</th>\n",
       "      <td>83</td>\n",
       "      <td>88</td>\n",
       "      <td>57</td>\n",
       "      <td>75</td>\n",
       "      <td>101</td>\n",
       "      <td>37</td>\n",
       "      <td>17</td>\n",
       "      <td>63</td>\n",
       "      <td>69</td>\n",
       "      <td>74</td>\n",
       "      <td>...</td>\n",
       "      <td>87</td>\n",
       "      <td>71</td>\n",
       "      <td>52</td>\n",
       "      <td>95</td>\n",
       "      <td>183</td>\n",
       "      <td>176</td>\n",
       "      <td>180</td>\n",
       "      <td>182</td>\n",
       "      <td>184</td>\n",
       "      <td>186</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 784 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  pixel9  \\\n",
       "22706      97      65     128      44      17      67      64      66      69   \n",
       "1231      215     216     217     223     206     176     176     191     106   \n",
       "531       144     145     146     147     150     150     151     151     151   \n",
       "21099      64      73      83      96     101     108     111     115     118   \n",
       "17752      83      88      57      75     101      37      17      63      69   \n",
       "\n",
       "       pixel10    ...     pixel775  pixel776  pixel777  pixel778  pixel779  \\\n",
       "22706       72    ...          189       180       134        97        76   \n",
       "1231       122    ...          133       133       132       132       134   \n",
       "531        150    ...           78       107       183       187       171   \n",
       "21099      122    ...          163       167       169       169       170   \n",
       "17752       74    ...           87        71        52        95       183   \n",
       "\n",
       "       pixel780  pixel781  pixel782  pixel783  pixel784  \n",
       "22706        86        94        90        91        91  \n",
       "1231        124       167       124        23       140  \n",
       "531         160       131       134       101        72  \n",
       "21099       173       173       175       175       176  \n",
       "17752       176       180       182       184       186  \n",
       "\n",
       "[5 rows x 784 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_tr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    21964.000000\n",
       "mean        12.324121\n",
       "std          7.298931\n",
       "min          0.000000\n",
       "25%          6.000000\n",
       "50%         13.000000\n",
       "75%         19.000000\n",
       "max         24.000000\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_tr.shape\n",
    "#y_tr.info()\n",
    "y_tr.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22706    10\n",
       "1231     15\n",
       "531       0\n",
       "21099    10\n",
       "17752    22\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_tr.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "(b) Construya una función que escale apropiadamente las imágenes antes de trabajar. Experimente sólo\n",
    "escalando los datos de acuerdo a la intensidad máxima de pixel (i.e., dividiendo por 255) y luego\n",
    "centrando y escalándolos como en actividades anteriores.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.1761998  -2.09806037 -0.60012897 -2.85449181 -3.76039244 -2.53284031\n",
      "  -2.7583446  -2.86682037 -2.90983062 -2.99411405]\n",
      " [ 1.67937834  1.68848848  1.68209134  1.8015662   1.33950431  0.48377907\n",
      "   0.44027188  0.8488636  -1.77696957 -1.39475919]\n",
      " [-0.03880851 -0.09194177 -0.13855632 -0.17530759 -0.17157621 -0.23578152\n",
      "  -0.27370501 -0.34015527 -0.39916559 -0.49912047]\n",
      " [-1.97479369 -1.89744851 -1.75406059 -1.50189395 -1.49377166 -1.39814862\n",
      "  -1.41606804 -1.41027225 -1.40955517 -1.39475919]\n",
      " [-1.51499721 -1.52130128 -2.42077663 -2.0481354  -1.49377166 -3.36310253\n",
      "  -4.10062115 -2.95599679 -2.90983062 -2.93013986]\n",
      " [ 0.73558556  0.68542918  0.65637435  0.60503733  0.55698047  0.45610366\n",
      "   0.4117128   0.40298152  0.39689894  0.33254406]\n",
      " [ 0.20318964  0.28420546  0.32301633  0.31891086  0.34111182  0.34540203\n",
      "   0.38315373  0.40298152  0.39689894  0.42850535]\n",
      " [ 0.56618686  0.56004677  0.52815973  0.50099134  0.44904614  0.40075285\n",
      "   0.38315373  0.34353058  0.33566321  0.30055696]\n",
      " [-0.32920629 -0.3427066  -0.34369972 -0.35738807 -0.36046128 -0.40183396\n",
      "  -0.38794131 -0.36988074 -0.39916559 -0.43514627]\n",
      " [ 1.05018316  1.06157642  1.04101822  0.99520979  0.96173418  0.92658558\n",
      "   0.89721709  0.90831455  0.8867848   0.94029891]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\"\"\"\n",
    "std = StandardScaler(with_mean=True, with_std=True)\n",
    "std.fit(X_train)\n",
    "X_train = std.transform(X_train)\n",
    "X_test = std.transform(X_test)\n",
    "\"\"\"\n",
    "def escalador(x_tr, x_v, x_t,n=255.0):\n",
    "    #Se regula la intensidad de todos por igual\n",
    "    x_tr = np.divide(x_tr, float(n))\n",
    "    x_v = np.divide(x_v, float(n))\n",
    "    x_t = np.divide(x_t, float(n))\n",
    "    #Se centran y escalan todos los datos usando como referencia el de entrenamiento\n",
    "    #de la misma manera que en el entregable anterior\n",
    "    std = StandardScaler(with_mean=True, with_std=True)\n",
    "    std.fit(x_tr)\n",
    "    X_train = std.transform(x_tr)\n",
    "    X_val = std.transform(x_v)\n",
    "    X_test = std.transform(x_t)\n",
    "    \n",
    "    return(X_train, X_val, X_test)\n",
    "Sx_tr,Sx_v,Sx_t = escalador(x_tr,x_v,x_t)#con n 255 por defecto\n",
    "print(Sx_tr[:10,:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "(c) Diseñe, entrene y evalúe una red neuronal para el problema partir de la representación original de las\n",
    "imágenes. Experimente con distintas arquitecturas, pre-procesamientos y métodos de entrenamiento,\n",
    "midiendo el error de clasificación sobre el conjunto de validación. En base a esta última medida de\n",
    "desempeño, decida qué modelo, de entre todos los evaluados, medirá finalmente en el conjunto de test.\n",
    "Reporte y discuta los resultados obtenidos. Se espera que logre obtener un error de pruebas menor o\n",
    "igual a 0.2.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alfredo\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(30, input_dim=784, activation=\"relu\", kernel_initializer=\"uniform\")`\n",
      "  import sys\n",
      "C:\\Users\\Alfredo\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(30, activation=\"relu\", kernel_initializer=\"uniform\")`\n",
      "  \n",
      "C:\\Users\\Alfredo\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(25, activation=\"softmax\", kernel_initializer=\"uniform\")`\n",
      "  if __name__ == '__main__':\n",
      "C:\\Users\\Alfredo\\Anaconda3\\lib\\site-packages\\keras\\models.py:939: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 21964 samples, validate on 5491 samples\n",
      "Epoch 1/100\n",
      "21964/21964 [==============================] - 1s 64us/step - loss: 15.3576 - acc: 0.0409 - val_loss: 15.5105 - val_acc: 0.0377\n",
      "Epoch 2/100\n",
      "21964/21964 [==============================] - 1s 50us/step - loss: 15.4496 - acc: 0.0415 - val_loss: 15.5105 - val_acc: 0.0377\n",
      "Epoch 3/100\n",
      "21964/21964 [==============================] - 1s 50us/step - loss: 15.4496 - acc: 0.0415 - val_loss: 15.5105 - val_acc: 0.0377\n",
      "Epoch 4/100\n",
      "21964/21964 [==============================] - 1s 52us/step - loss: 15.4496 - acc: 0.0415 - val_loss: 15.5105 - val_acc: 0.0377\n",
      "Epoch 5/100\n",
      "21964/21964 [==============================] - 1s 49us/step - loss: 15.4496 - acc: 0.0415 - val_loss: 15.5105 - val_acc: 0.0377\n",
      "Epoch 6/100\n",
      "21964/21964 [==============================] - 1s 50us/step - loss: 15.4496 - acc: 0.0415 - val_loss: 15.5105 - val_acc: 0.0377\n",
      "Epoch 7/100\n",
      "21964/21964 [==============================] - 1s 48us/step - loss: 15.4496 - acc: 0.0415 - val_loss: 15.5105 - val_acc: 0.0377\n",
      "Epoch 8/100\n",
      "21964/21964 [==============================] - 1s 49us/step - loss: 15.4496 - acc: 0.0415 - val_loss: 15.5105 - val_acc: 0.0377\n",
      "Epoch 9/100\n",
      "21964/21964 [==============================] - 1s 50us/step - loss: 15.4496 - acc: 0.0415 - val_loss: 15.5105 - val_acc: 0.0377\n",
      "Epoch 10/100\n",
      "21964/21964 [==============================] - 1s 49us/step - loss: 15.4496 - acc: 0.0415 - val_loss: 15.5105 - val_acc: 0.0377\n",
      "Epoch 11/100\n",
      "21964/21964 [==============================] - 1s 49us/step - loss: 15.4496 - acc: 0.0415 - val_loss: 15.5105 - val_acc: 0.0377\n",
      "Epoch 12/100\n",
      "21964/21964 [==============================] - 1s 50us/step - loss: 15.4496 - acc: 0.0415 - val_loss: 15.5105 - val_acc: 0.0377\n",
      "Epoch 13/100\n",
      "21964/21964 [==============================] - 1s 50us/step - loss: 15.4496 - acc: 0.0415 - val_loss: 15.5105 - val_acc: 0.0377\n",
      "Epoch 14/100\n",
      "21964/21964 [==============================] - 1s 50us/step - loss: 15.4496 - acc: 0.0415 - val_loss: 15.5105 - val_acc: 0.0377\n",
      "Epoch 15/100\n",
      "21964/21964 [==============================] - 1s 53us/step - loss: 15.4496 - acc: 0.0415 - val_loss: 15.5105 - val_acc: 0.0377\n",
      "Epoch 16/100\n",
      "21964/21964 [==============================] - 1s 50us/step - loss: 15.4496 - acc: 0.0415 - val_loss: 15.5105 - val_acc: 0.0377\n",
      "Epoch 17/100\n",
      "21964/21964 [==============================] - 1s 50us/step - loss: 15.4496 - acc: 0.0415 - val_loss: 15.5105 - val_acc: 0.0377\n",
      "Epoch 18/100\n",
      "21964/21964 [==============================] - 1s 52us/step - loss: 15.4496 - acc: 0.0415 - val_loss: 15.5105 - val_acc: 0.0377\n",
      "Epoch 19/100\n",
      "21964/21964 [==============================] - 1s 56us/step - loss: 15.4496 - acc: 0.0415 - val_loss: 15.5105 - val_acc: 0.0377\n",
      "Epoch 20/100\n",
      "21964/21964 [==============================] - 1s 50us/step - loss: 15.4496 - acc: 0.0415 - val_loss: 15.5105 - val_acc: 0.0377\n",
      "Epoch 21/100\n",
      "21964/21964 [==============================] - 1s 51us/step - loss: 15.4496 - acc: 0.0415 - val_loss: 15.5105 - val_acc: 0.0377\n",
      "Epoch 22/100\n",
      "21964/21964 [==============================] - 1s 50us/step - loss: 15.4496 - acc: 0.0415 - val_loss: 15.5105 - val_acc: 0.0377\n",
      "Epoch 23/100\n",
      "21964/21964 [==============================] - 1s 50us/step - loss: 15.4496 - acc: 0.0415 - val_loss: 15.5105 - val_acc: 0.0377\n",
      "Epoch 24/100\n",
      "21964/21964 [==============================] - 1s 51us/step - loss: 15.4496 - acc: 0.0415 - val_loss: 15.5105 - val_acc: 0.0377\n",
      "Epoch 25/100\n",
      "21964/21964 [==============================] - 1s 51us/step - loss: 15.4496 - acc: 0.0415 - val_loss: 15.5105 - val_acc: 0.0377\n",
      "Epoch 26/100\n",
      "21964/21964 [==============================] - 1s 54us/step - loss: 15.4496 - acc: 0.0415 - val_loss: 15.5105 - val_acc: 0.0377\n",
      "Epoch 27/100\n",
      "21964/21964 [==============================] - 1s 51us/step - loss: 15.4496 - acc: 0.0415 - val_loss: 15.5105 - val_acc: 0.0377\n",
      "Epoch 28/100\n",
      "21964/21964 [==============================] - 1s 50us/step - loss: 15.4496 - acc: 0.0415 - val_loss: 15.5105 - val_acc: 0.0377\n",
      "Epoch 29/100\n",
      "21964/21964 [==============================] - 1s 49us/step - loss: 15.4496 - acc: 0.0415 - val_loss: 15.5105 - val_acc: 0.0377\n",
      "Epoch 30/100\n",
      "21964/21964 [==============================] - 1s 51us/step - loss: 15.4496 - acc: 0.0415 - val_loss: 15.5105 - val_acc: 0.0377\n",
      "Epoch 31/100\n",
      "21964/21964 [==============================] - 1s 52us/step - loss: 15.4496 - acc: 0.0415 - val_loss: 15.5105 - val_acc: 0.0377\n",
      "Epoch 32/100\n",
      "21964/21964 [==============================] - 1s 52us/step - loss: 15.4496 - acc: 0.0415 - val_loss: 15.5105 - val_acc: 0.0377\n",
      "Epoch 33/100\n",
      "21964/21964 [==============================] - 1s 50us/step - loss: 15.4496 - acc: 0.0415 - val_loss: 15.5105 - val_acc: 0.0377\n",
      "Epoch 34/100\n",
      "21964/21964 [==============================] - 1s 51us/step - loss: 15.4496 - acc: 0.0415 - val_loss: 15.5105 - val_acc: 0.0377\n",
      "Epoch 35/100\n",
      "21964/21964 [==============================] - 1s 51us/step - loss: 15.4496 - acc: 0.0415 - val_loss: 15.5105 - val_acc: 0.0377\n",
      "Epoch 36/100\n",
      "21964/21964 [==============================] - 1s 53us/step - loss: 15.4496 - acc: 0.0415 - val_loss: 15.5105 - val_acc: 0.0377\n",
      "Epoch 37/100\n",
      "21964/21964 [==============================] - 1s 51us/step - loss: 15.4496 - acc: 0.0415 - val_loss: 15.5105 - val_acc: 0.0377\n",
      "Epoch 38/100\n",
      "21964/21964 [==============================] - 1s 50us/step - loss: 15.4496 - acc: 0.0415 - val_loss: 15.5105 - val_acc: 0.0377\n",
      "Epoch 39/100\n",
      "21964/21964 [==============================] - 1s 52us/step - loss: 15.4496 - acc: 0.0415 - val_loss: 15.5105 - val_acc: 0.0377\n",
      "Epoch 40/100\n",
      "21964/21964 [==============================] - 1s 52us/step - loss: 15.4496 - acc: 0.0415 - val_loss: 15.5105 - val_acc: 0.0377\n",
      "Epoch 41/100\n",
      "21964/21964 [==============================] - 1s 52us/step - loss: 15.4496 - acc: 0.0415 - val_loss: 15.5105 - val_acc: 0.0377\n",
      "Epoch 42/100\n",
      "21964/21964 [==============================] - 1s 52us/step - loss: 15.4496 - acc: 0.0415 - val_loss: 15.5105 - val_acc: 0.0377\n",
      "Epoch 43/100\n",
      "21964/21964 [==============================] - 1s 52us/step - loss: 15.4496 - acc: 0.0415 - val_loss: 15.5105 - val_acc: 0.0377\n",
      "Epoch 44/100\n",
      "21964/21964 [==============================] - 1s 52us/step - loss: 15.4496 - acc: 0.0415 - val_loss: 15.5105 - val_acc: 0.0377\n",
      "Epoch 45/100\n",
      "21964/21964 [==============================] - 1s 52us/step - loss: 15.4496 - acc: 0.0415 - val_loss: 15.5105 - val_acc: 0.0377\n",
      "Epoch 46/100\n",
      "21964/21964 [==============================] - 1s 51us/step - loss: 15.4496 - acc: 0.0415 - val_loss: 15.5105 - val_acc: 0.0377\n",
      "Epoch 47/100\n",
      "21964/21964 [==============================] - 1s 50us/step - loss: 15.4496 - acc: 0.0415 - val_loss: 15.5105 - val_acc: 0.0377\n",
      "Epoch 48/100\n",
      "21964/21964 [==============================] - 1s 55us/step - loss: 15.4496 - acc: 0.0415 - val_loss: 15.5105 - val_acc: 0.0377\n",
      "Epoch 49/100\n",
      "21964/21964 [==============================] - 1s 55us/step - loss: 15.4496 - acc: 0.0415 - val_loss: 15.5105 - val_acc: 0.0377\n",
      "Epoch 50/100\n",
      "21964/21964 [==============================] - 1s 55us/step - loss: 15.4496 - acc: 0.0415 - val_loss: 15.5105 - val_acc: 0.0377\n",
      "Epoch 51/100\n",
      "21964/21964 [==============================] - 1s 53us/step - loss: 15.4496 - acc: 0.0415 - val_loss: 15.5105 - val_acc: 0.0377\n",
      "Epoch 52/100\n",
      "21964/21964 [==============================] - 1s 52us/step - loss: 15.4496 - acc: 0.0415 - val_loss: 15.5105 - val_acc: 0.0377\n",
      "Epoch 53/100\n",
      "21964/21964 [==============================] - 1s 57us/step - loss: 15.4496 - acc: 0.0415 - val_loss: 15.5105 - val_acc: 0.0377\n",
      "Epoch 54/100\n",
      "21964/21964 [==============================] - 1s 54us/step - loss: 15.4496 - acc: 0.0415 - val_loss: 15.5105 - val_acc: 0.0377\n",
      "Epoch 55/100\n",
      "21964/21964 [==============================] - 1s 54us/step - loss: 15.4496 - acc: 0.0415 - val_loss: 15.5105 - val_acc: 0.0377\n",
      "Epoch 56/100\n",
      "21964/21964 [==============================] - 1s 53us/step - loss: 15.4496 - acc: 0.0415 - val_loss: 15.5105 - val_acc: 0.0377\n",
      "Epoch 57/100\n",
      "21964/21964 [==============================] - 1s 53us/step - loss: 15.4496 - acc: 0.0415 - val_loss: 15.5105 - val_acc: 0.0377\n",
      "Epoch 58/100\n",
      "21964/21964 [==============================] - 1s 52us/step - loss: 15.4496 - acc: 0.0415 - val_loss: 15.5105 - val_acc: 0.0377\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/100\n",
      "21964/21964 [==============================] - 1s 49us/step - loss: 15.4496 - acc: 0.0415 - val_loss: 15.5105 - val_acc: 0.0377\n",
      "Epoch 60/100\n",
      "21964/21964 [==============================] - 1s 49us/step - loss: 15.4496 - acc: 0.0415 - val_loss: 15.5105 - val_acc: 0.0377\n",
      "Epoch 61/100\n",
      "21964/21964 [==============================] - 1s 49us/step - loss: 15.4496 - acc: 0.0415 - val_loss: 15.5105 - val_acc: 0.0377\n",
      "Epoch 62/100\n",
      "21964/21964 [==============================] - 1s 48us/step - loss: 15.4496 - acc: 0.0415 - val_loss: 15.5105 - val_acc: 0.0377\n",
      "Epoch 63/100\n",
      "21964/21964 [==============================] - 1s 52us/step - loss: 15.4496 - acc: 0.0415 - val_loss: 15.5105 - val_acc: 0.0377\n",
      "Epoch 64/100\n",
      "21964/21964 [==============================] - 1s 54us/step - loss: 15.4496 - acc: 0.0415 - val_loss: 15.5105 - val_acc: 0.0377\n",
      "Epoch 65/100\n",
      "21964/21964 [==============================] - 1s 50us/step - loss: 15.4496 - acc: 0.0415 - val_loss: 15.5105 - val_acc: 0.0377\n",
      "Epoch 66/100\n",
      "21964/21964 [==============================] - 1s 50us/step - loss: 15.4496 - acc: 0.0415 - val_loss: 15.5105 - val_acc: 0.0377\n",
      "Epoch 67/100\n",
      "21964/21964 [==============================] - 1s 50us/step - loss: 15.4496 - acc: 0.0415 - val_loss: 15.5105 - val_acc: 0.0377\n",
      "Epoch 68/100\n",
      "21964/21964 [==============================] - 1s 49us/step - loss: 15.4496 - acc: 0.0415 - val_loss: 15.5105 - val_acc: 0.0377\n",
      "Epoch 69/100\n",
      "21964/21964 [==============================] - 1s 49us/step - loss: 15.4496 - acc: 0.0415 - val_loss: 15.5105 - val_acc: 0.0377\n",
      "Epoch 70/100\n",
      "21964/21964 [==============================] - 1s 50us/step - loss: 15.4496 - acc: 0.0415 - val_loss: 15.5105 - val_acc: 0.0377\n",
      "Epoch 71/100\n",
      "21964/21964 [==============================] - 1s 49us/step - loss: 15.4496 - acc: 0.0415 - val_loss: 15.5105 - val_acc: 0.0377\n",
      "Epoch 72/100\n",
      "21964/21964 [==============================] - 1s 49us/step - loss: 15.4496 - acc: 0.0415 - val_loss: 15.5105 - val_acc: 0.0377\n",
      "Epoch 73/100\n",
      "21964/21964 [==============================] - 1s 50us/step - loss: 15.4496 - acc: 0.0415 - val_loss: 15.5105 - val_acc: 0.0377\n",
      "Epoch 74/100\n",
      "21964/21964 [==============================] - 1s 49us/step - loss: 15.4496 - acc: 0.0415 - val_loss: 15.5105 - val_acc: 0.0377\n",
      "Epoch 75/100\n",
      "21964/21964 [==============================] - 1s 50us/step - loss: 15.4496 - acc: 0.0415 - val_loss: 15.5105 - val_acc: 0.0377\n",
      "Epoch 76/100\n",
      "21964/21964 [==============================] - 1s 52us/step - loss: 15.4496 - acc: 0.0415 - val_loss: 15.5105 - val_acc: 0.0377\n",
      "Epoch 77/100\n",
      "21964/21964 [==============================] - 1s 54us/step - loss: 15.4496 - acc: 0.0415 - val_loss: 15.5105 - val_acc: 0.0377\n",
      "Epoch 78/100\n",
      "21964/21964 [==============================] - 1s 50us/step - loss: 15.4496 - acc: 0.0415 - val_loss: 15.5105 - val_acc: 0.0377\n",
      "Epoch 79/100\n",
      "21964/21964 [==============================] - 1s 49us/step - loss: 15.4496 - acc: 0.0415 - val_loss: 15.5105 - val_acc: 0.0377\n",
      "Epoch 80/100\n",
      "21964/21964 [==============================] - 1s 50us/step - loss: 15.4496 - acc: 0.0415 - val_loss: 15.5105 - val_acc: 0.0377\n",
      "Epoch 81/100\n",
      "21964/21964 [==============================] - 1s 49us/step - loss: 15.4496 - acc: 0.0415 - val_loss: 15.5105 - val_acc: 0.0377\n",
      "Epoch 82/100\n",
      "21964/21964 [==============================] - 1s 49us/step - loss: 15.4496 - acc: 0.0415 - val_loss: 15.5105 - val_acc: 0.0377\n",
      "Epoch 83/100\n",
      "21964/21964 [==============================] - 1s 50us/step - loss: 15.4496 - acc: 0.0415 - val_loss: 15.5105 - val_acc: 0.0377\n",
      "Epoch 84/100\n",
      "21964/21964 [==============================] - 1s 52us/step - loss: 15.4496 - acc: 0.0415 - val_loss: 15.5105 - val_acc: 0.0377\n",
      "Epoch 85/100\n",
      "21964/21964 [==============================] - 1s 54us/step - loss: 15.4496 - acc: 0.0415 - val_loss: 15.5105 - val_acc: 0.0377\n",
      "Epoch 86/100\n",
      "21964/21964 [==============================] - 1s 56us/step - loss: 15.4496 - acc: 0.0415 - val_loss: 15.5105 - val_acc: 0.0377\n",
      "Epoch 87/100\n",
      "21964/21964 [==============================] - 1s 53us/step - loss: 15.4496 - acc: 0.0415 - val_loss: 15.5105 - val_acc: 0.0377\n",
      "Epoch 88/100\n",
      "21964/21964 [==============================] - 1s 53us/step - loss: 15.4496 - acc: 0.0415 - val_loss: 15.5105 - val_acc: 0.0377\n",
      "Epoch 89/100\n",
      "21964/21964 [==============================] - 1s 53us/step - loss: 15.4496 - acc: 0.0415 - val_loss: 15.5105 - val_acc: 0.0377\n",
      "Epoch 90/100\n",
      "21964/21964 [==============================] - 1s 52us/step - loss: 15.4496 - acc: 0.0415 - val_loss: 15.5105 - val_acc: 0.0377\n",
      "Epoch 91/100\n",
      "21964/21964 [==============================] - 1s 52us/step - loss: 15.4496 - acc: 0.0415 - val_loss: 15.5105 - val_acc: 0.0377\n",
      "Epoch 92/100\n",
      "21964/21964 [==============================] - 1s 53us/step - loss: 15.4496 - acc: 0.0415 - val_loss: 15.5105 - val_acc: 0.0377\n",
      "Epoch 93/100\n",
      "21964/21964 [==============================] - 1s 52us/step - loss: 15.4496 - acc: 0.0415 - val_loss: 15.5105 - val_acc: 0.0377\n",
      "Epoch 94/100\n",
      "21964/21964 [==============================] - 1s 51us/step - loss: 15.4496 - acc: 0.0415 - val_loss: 15.5105 - val_acc: 0.0377\n",
      "Epoch 95/100\n",
      "21964/21964 [==============================] - 1s 51us/step - loss: 15.4496 - acc: 0.0415 - val_loss: 15.5105 - val_acc: 0.0377\n",
      "Epoch 96/100\n",
      "21964/21964 [==============================] - 1s 52us/step - loss: 15.4496 - acc: 0.0415 - val_loss: 15.5105 - val_acc: 0.0377\n",
      "Epoch 97/100\n",
      "21964/21964 [==============================] - 1s 52us/step - loss: 15.4496 - acc: 0.0415 - val_loss: 15.5105 - val_acc: 0.0377\n",
      "Epoch 98/100\n",
      "21964/21964 [==============================] - 1s 52us/step - loss: 15.4496 - acc: 0.0415 - val_loss: 15.5105 - val_acc: 0.0377\n",
      "Epoch 99/100\n",
      "21964/21964 [==============================] - 1s 51us/step - loss: 15.4496 - acc: 0.0415 - val_loss: 15.5105 - val_acc: 0.0377\n",
      "Epoch 100/100\n",
      "21964/21964 [==============================] - 1s 54us/step - loss: 15.4496 - acc: 0.0415 - val_loss: 15.5105 - val_acc: 0.0377\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2af83a94a20>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils.np_utils import to_categorical\n",
    "#Con la data sin pre-procesar\n",
    "model = Sequential()\n",
    "model.add(Dense(30, input_dim=x_tr.shape[1], init='uniform', activation='relu'))\n",
    "model.add(Dense(30, init='uniform', activation='relu'))\n",
    "model.add(Dense(25, init='uniform', activation='softmax'))\n",
    "model.compile(optimizer=SGD(lr=0.05), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(x_tr.values, to_categorical(y_tr), nb_epoch=100, batch_size=128, verbose=1,\n",
    "                        validation_data=(x_v.values,to_categorical(y_v)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alfredo\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(30, input_dim=784, activation=\"relu\", kernel_initializer=\"uniform\")`\n",
      "  import sys\n",
      "C:\\Users\\Alfredo\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(25, activation=\"softmax\", kernel_initializer=\"uniform\")`\n",
      "  \n",
      "C:\\Users\\Alfredo\\Anaconda3\\lib\\site-packages\\keras\\models.py:939: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 21964 samples, validate on 5491 samples\n",
      "Epoch 1/50\n",
      "21964/21964 [==============================] - 1s 68us/step - loss: 15.5046 - acc: 0.0373 - val_loss: 15.5663 - val_acc: 0.0342\n",
      "Epoch 2/50\n",
      "21964/21964 [==============================] - 1s 48us/step - loss: 15.5149 - acc: 0.0374 - val_loss: 15.5663 - val_acc: 0.0342\n",
      "Epoch 3/50\n",
      "21964/21964 [==============================] - 1s 49us/step - loss: 15.5149 - acc: 0.0374 - val_loss: 15.5663 - val_acc: 0.0342\n",
      "Epoch 4/50\n",
      "21964/21964 [==============================] - 1s 48us/step - loss: 15.5149 - acc: 0.0374 - val_loss: 15.5663 - val_acc: 0.0342\n",
      "Epoch 5/50\n",
      "21964/21964 [==============================] - 1s 48us/step - loss: 15.5149 - acc: 0.0374 - val_loss: 15.5663 - val_acc: 0.0342\n",
      "Epoch 6/50\n",
      "21964/21964 [==============================] - 1s 49us/step - loss: 15.5149 - acc: 0.0374 - val_loss: 15.5663 - val_acc: 0.0342\n",
      "Epoch 7/50\n",
      "21964/21964 [==============================] - 1s 53us/step - loss: 15.5149 - acc: 0.0374 - val_loss: 15.5663 - val_acc: 0.0342\n",
      "Epoch 8/50\n",
      "21964/21964 [==============================] - 1s 55us/step - loss: 15.5149 - acc: 0.0374 - val_loss: 15.5663 - val_acc: 0.0342\n",
      "Epoch 9/50\n",
      "21964/21964 [==============================] - 1s 50us/step - loss: 15.5149 - acc: 0.0374 - val_loss: 15.5663 - val_acc: 0.0342\n",
      "Epoch 10/50\n",
      "21964/21964 [==============================] - 1s 49us/step - loss: 15.5149 - acc: 0.0374 - val_loss: 15.5663 - val_acc: 0.0342\n",
      "Epoch 11/50\n",
      "21964/21964 [==============================] - 1s 55us/step - loss: 15.5149 - acc: 0.0374 - val_loss: 15.5663 - val_acc: 0.0342\n",
      "Epoch 12/50\n",
      "21964/21964 [==============================] - 1s 57us/step - loss: 15.5149 - acc: 0.0374 - val_loss: 15.5663 - val_acc: 0.0342\n",
      "Epoch 13/50\n",
      "21964/21964 [==============================] - 1s 49us/step - loss: 15.5149 - acc: 0.0374 - val_loss: 15.5663 - val_acc: 0.0342\n",
      "Epoch 14/50\n",
      "21964/21964 [==============================] - 1s 52us/step - loss: 15.5149 - acc: 0.0374 - val_loss: 15.5663 - val_acc: 0.0342\n",
      "Epoch 15/50\n",
      "21964/21964 [==============================] - 1s 52us/step - loss: 15.5149 - acc: 0.0374 - val_loss: 15.5663 - val_acc: 0.0342\n",
      "Epoch 16/50\n",
      "21964/21964 [==============================] - 1s 48us/step - loss: 15.5149 - acc: 0.0374 - val_loss: 15.5663 - val_acc: 0.0342\n",
      "Epoch 17/50\n",
      "21964/21964 [==============================] - 1s 49us/step - loss: 15.5149 - acc: 0.0374 - val_loss: 15.5663 - val_acc: 0.0342\n",
      "Epoch 18/50\n",
      "21964/21964 [==============================] - 1s 50us/step - loss: 15.5149 - acc: 0.0374 - val_loss: 15.5663 - val_acc: 0.0342\n",
      "Epoch 19/50\n",
      "21964/21964 [==============================] - 1s 56us/step - loss: 15.5149 - acc: 0.0374 - val_loss: 15.5663 - val_acc: 0.0342\n",
      "Epoch 20/50\n",
      "21964/21964 [==============================] - 2s 68us/step - loss: 15.5149 - acc: 0.0374 - val_loss: 15.5663 - val_acc: 0.0342\n",
      "Epoch 21/50\n",
      "21964/21964 [==============================] - 1s 58us/step - loss: 15.5149 - acc: 0.0374 - val_loss: 15.5663 - val_acc: 0.0342\n",
      "Epoch 22/50\n",
      "21964/21964 [==============================] - 1s 57us/step - loss: 15.5149 - acc: 0.0374 - val_loss: 15.5663 - val_acc: 0.0342\n",
      "Epoch 23/50\n",
      "21964/21964 [==============================] - 1s 54us/step - loss: 15.5149 - acc: 0.0374 - val_loss: 15.5663 - val_acc: 0.0342\n",
      "Epoch 24/50\n",
      "21964/21964 [==============================] - 1s 54us/step - loss: 15.5149 - acc: 0.0374 - val_loss: 15.5663 - val_acc: 0.0342\n",
      "Epoch 25/50\n",
      "21964/21964 [==============================] - 1s 53us/step - loss: 15.5149 - acc: 0.0374 - val_loss: 15.5663 - val_acc: 0.0342\n",
      "Epoch 26/50\n",
      "21964/21964 [==============================] - 1s 45us/step - loss: 15.5149 - acc: 0.0374 - val_loss: 15.5663 - val_acc: 0.0342\n",
      "Epoch 27/50\n",
      "21964/21964 [==============================] - 1s 45us/step - loss: 15.5149 - acc: 0.0374 - val_loss: 15.5663 - val_acc: 0.0342\n",
      "Epoch 28/50\n",
      "21964/21964 [==============================] - 1s 45us/step - loss: 15.5149 - acc: 0.0374 - val_loss: 15.5663 - val_acc: 0.0342\n",
      "Epoch 29/50\n",
      "21964/21964 [==============================] - 1s 47us/step - loss: 15.5149 - acc: 0.0374 - val_loss: 15.5663 - val_acc: 0.0342\n",
      "Epoch 30/50\n",
      "21964/21964 [==============================] - 1s 57us/step - loss: 15.5149 - acc: 0.0374 - val_loss: 15.5663 - val_acc: 0.0342\n",
      "Epoch 31/50\n",
      "21964/21964 [==============================] - 1s 52us/step - loss: 15.5149 - acc: 0.0374 - val_loss: 15.5663 - val_acc: 0.0342\n",
      "Epoch 32/50\n",
      "21964/21964 [==============================] - 1s 46us/step - loss: 15.5149 - acc: 0.0374 - val_loss: 15.5663 - val_acc: 0.0342\n",
      "Epoch 33/50\n",
      "21964/21964 [==============================] - 1s 48us/step - loss: 15.5149 - acc: 0.0374 - val_loss: 15.5663 - val_acc: 0.0342\n",
      "Epoch 34/50\n",
      "21964/21964 [==============================] - 1s 45us/step - loss: 15.5149 - acc: 0.0374 - val_loss: 15.5663 - val_acc: 0.0342\n",
      "Epoch 35/50\n",
      "21964/21964 [==============================] - 1s 51us/step - loss: 15.5149 - acc: 0.0374 - val_loss: 15.5663 - val_acc: 0.0342\n",
      "Epoch 36/50\n",
      "21964/21964 [==============================] - 1s 50us/step - loss: 15.5149 - acc: 0.0374 - val_loss: 15.5663 - val_acc: 0.0342\n",
      "Epoch 37/50\n",
      "21964/21964 [==============================] - 1s 46us/step - loss: 15.5149 - acc: 0.0374 - val_loss: 15.5663 - val_acc: 0.0342\n",
      "Epoch 38/50\n",
      "21964/21964 [==============================] - 1s 46us/step - loss: 15.5149 - acc: 0.0374 - val_loss: 15.5663 - val_acc: 0.0342\n",
      "Epoch 39/50\n",
      "21964/21964 [==============================] - 1s 45us/step - loss: 15.5149 - acc: 0.0374 - val_loss: 15.5663 - val_acc: 0.0342\n",
      "Epoch 40/50\n",
      "21964/21964 [==============================] - 1s 51us/step - loss: 15.5149 - acc: 0.0374 - val_loss: 15.5663 - val_acc: 0.0342\n",
      "Epoch 41/50\n",
      "21964/21964 [==============================] - 1s 47us/step - loss: 15.5149 - acc: 0.0374 - val_loss: 15.5663 - val_acc: 0.0342\n",
      "Epoch 42/50\n",
      "21964/21964 [==============================] - 1s 45us/step - loss: 15.5149 - acc: 0.0374 - val_loss: 15.5663 - val_acc: 0.0342\n",
      "Epoch 43/50\n",
      "21964/21964 [==============================] - 1s 49us/step - loss: 15.5149 - acc: 0.0374 - val_loss: 15.5663 - val_acc: 0.0342\n",
      "Epoch 44/50\n",
      "21964/21964 [==============================] - 1s 51us/step - loss: 15.5149 - acc: 0.0374 - val_loss: 15.5663 - val_acc: 0.0342\n",
      "Epoch 45/50\n",
      "21964/21964 [==============================] - 1s 46us/step - loss: 15.5149 - acc: 0.0374 - val_loss: 15.5663 - val_acc: 0.0342\n",
      "Epoch 46/50\n",
      "21964/21964 [==============================] - 1s 47us/step - loss: 15.5149 - acc: 0.0374 - val_loss: 15.5663 - val_acc: 0.0342\n",
      "Epoch 47/50\n",
      "21964/21964 [==============================] - 1s 50us/step - loss: 15.5149 - acc: 0.0374 - val_loss: 15.5663 - val_acc: 0.0342\n",
      "Epoch 48/50\n",
      "21964/21964 [==============================] - 1s 46us/step - loss: 15.5149 - acc: 0.0374 - val_loss: 15.5663 - val_acc: 0.0342\n",
      "Epoch 49/50\n",
      "21964/21964 [==============================] - 1s 45us/step - loss: 15.5149 - acc: 0.0374 - val_loss: 15.5663 - val_acc: 0.0342\n",
      "Epoch 50/50\n",
      "21964/21964 [==============================] - 1s 48us/step - loss: 15.5149 - acc: 0.0374 - val_loss: 15.5663 - val_acc: 0.0342\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2af83c44748>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils.np_utils import to_categorical\n",
    "#Con la data sin pre-procesar con una capa menos y  epoch a la mitad\n",
    "model1 = Sequential()\n",
    "model1.add(Dense(30, input_dim=x_tr.shape[1], init='uniform', activation='relu'))\n",
    "model1.add(Dense(25, init='uniform', activation='softmax'))\n",
    "model1.compile(optimizer=SGD(lr=0.05), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model1.fit(x_tr.values, to_categorical(y_tr), nb_epoch=50, batch_size=128, verbose=1,\n",
    "                        validation_data=(x_v.values,to_categorical(y_v)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alfredo\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(30, input_dim=784, activation=\"relu\", kernel_initializer=\"uniform\")`\n",
      "  import sys\n",
      "C:\\Users\\Alfredo\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(30, activation=\"elu\", kernel_initializer=\"uniform\")`\n",
      "  \n",
      "C:\\Users\\Alfredo\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(30, activation=\"tanh\", kernel_initializer=\"uniform\")`\n",
      "  if __name__ == '__main__':\n",
      "C:\\Users\\Alfredo\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:10: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(25, activation=\"softmax\", kernel_initializer=\"uniform\")`\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "C:\\Users\\Alfredo\\Anaconda3\\lib\\site-packages\\keras\\models.py:939: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 21964 samples, validate on 5491 samples\n",
      "Epoch 1/50\n",
      "21964/21964 [==============================] - 2s 70us/step - loss: 3.1546 - acc: 0.0695 - val_loss: 3.1914 - val_acc: 0.0475\n",
      "Epoch 2/50\n",
      "21964/21964 [==============================] - 1s 53us/step - loss: 3.1622 - acc: 0.0526 - val_loss: 3.1729 - val_acc: 0.0514\n",
      "Epoch 3/50\n",
      "21964/21964 [==============================] - 1s 52us/step - loss: 3.0047 - acc: 0.0953 - val_loss: 2.7607 - val_acc: 0.1218\n",
      "Epoch 4/50\n",
      "21964/21964 [==============================] - 1s 52us/step - loss: 2.8467 - acc: 0.1217 - val_loss: 2.6315 - val_acc: 0.1537\n",
      "Epoch 5/50\n",
      "21964/21964 [==============================] - 1s 52us/step - loss: 2.7190 - acc: 0.1436 - val_loss: 2.9165 - val_acc: 0.1076\n",
      "Epoch 6/50\n",
      "21964/21964 [==============================] - 1s 52us/step - loss: 2.6474 - acc: 0.1572 - val_loss: 2.6558 - val_acc: 0.1382\n",
      "Epoch 7/50\n",
      "21964/21964 [==============================] - 1s 53us/step - loss: 2.6351 - acc: 0.1599 - val_loss: 2.6570 - val_acc: 0.1353\n",
      "Epoch 8/50\n",
      "21964/21964 [==============================] - 1s 52us/step - loss: 2.7142 - acc: 0.1502 - val_loss: 3.1284 - val_acc: 0.0845\n",
      "Epoch 9/50\n",
      "21964/21964 [==============================] - 1s 52us/step - loss: 2.7302 - acc: 0.1450 - val_loss: 2.7364 - val_acc: 0.1380\n",
      "Epoch 10/50\n",
      "21964/21964 [==============================] - 1s 53us/step - loss: 2.6746 - acc: 0.1496 - val_loss: 2.6845 - val_acc: 0.1346\n",
      "Epoch 11/50\n",
      "21964/21964 [==============================] - 1s 53us/step - loss: 2.6708 - acc: 0.1517 - val_loss: 3.0056 - val_acc: 0.0925\n",
      "Epoch 12/50\n",
      "21964/21964 [==============================] - 1s 53us/step - loss: 3.0345 - acc: 0.0824 - val_loss: 3.1260 - val_acc: 0.0504\n",
      "Epoch 13/50\n",
      "21964/21964 [==============================] - 1s 52us/step - loss: 3.0030 - acc: 0.0843 - val_loss: 3.3367 - val_acc: 0.0421\n",
      "Epoch 14/50\n",
      "21964/21964 [==============================] - 1s 53us/step - loss: 3.1475 - acc: 0.0606 - val_loss: 2.9458 - val_acc: 0.1096\n",
      "Epoch 15/50\n",
      "21964/21964 [==============================] - 1s 53us/step - loss: 2.8872 - acc: 0.1257 - val_loss: 3.1469 - val_acc: 0.0799\n",
      "Epoch 16/50\n",
      "21964/21964 [==============================] - 1s 53us/step - loss: 3.1494 - acc: 0.0575 - val_loss: 3.1772 - val_acc: 0.0494\n",
      "Epoch 17/50\n",
      "21964/21964 [==============================] - 1s 53us/step - loss: 3.1796 - acc: 0.0427 - val_loss: 3.1783 - val_acc: 0.0475\n",
      "Epoch 18/50\n",
      "21964/21964 [==============================] - 1s 58us/step - loss: 3.1797 - acc: 0.0402 - val_loss: 3.1789 - val_acc: 0.0419\n",
      "Epoch 19/50\n",
      "21964/21964 [==============================] - 1s 57us/step - loss: 3.1794 - acc: 0.0432 - val_loss: 3.1789 - val_acc: 0.0488\n",
      "Epoch 20/50\n",
      "21964/21964 [==============================] - 1s 54us/step - loss: 3.1794 - acc: 0.0448 - val_loss: 3.1782 - val_acc: 0.0530\n",
      "Epoch 21/50\n",
      "21964/21964 [==============================] - 1s 54us/step - loss: 3.1792 - acc: 0.0421 - val_loss: 3.1777 - val_acc: 0.0488\n",
      "Epoch 22/50\n",
      "21964/21964 [==============================] - 1s 59us/step - loss: 3.1795 - acc: 0.0428 - val_loss: 3.1778 - val_acc: 0.0402\n",
      "Epoch 23/50\n",
      "21964/21964 [==============================] - 1s 58us/step - loss: 3.1792 - acc: 0.0459 - val_loss: 3.1780 - val_acc: 0.0530\n",
      "Epoch 24/50\n",
      "21964/21964 [==============================] - 1s 53us/step - loss: 3.1795 - acc: 0.0453 - val_loss: 3.1775 - val_acc: 0.0488\n",
      "Epoch 25/50\n",
      "21964/21964 [==============================] - 1s 57us/step - loss: 3.1791 - acc: 0.0429 - val_loss: 3.1788 - val_acc: 0.0441\n",
      "Epoch 26/50\n",
      "21964/21964 [==============================] - 1s 53us/step - loss: 3.1795 - acc: 0.0445 - val_loss: 3.1810 - val_acc: 0.0419\n",
      "Epoch 27/50\n",
      "21964/21964 [==============================] - 1s 52us/step - loss: 3.1793 - acc: 0.0429 - val_loss: 3.1783 - val_acc: 0.0441\n",
      "Epoch 28/50\n",
      "21964/21964 [==============================] - 1s 53us/step - loss: 3.1797 - acc: 0.0436 - val_loss: 3.1781 - val_acc: 0.0419\n",
      "Epoch 29/50\n",
      "21964/21964 [==============================] - 1s 53us/step - loss: 3.1790 - acc: 0.0427 - val_loss: 3.1774 - val_acc: 0.0494\n",
      "Epoch 30/50\n",
      "21964/21964 [==============================] - 1s 54us/step - loss: 3.1794 - acc: 0.0433 - val_loss: 3.1774 - val_acc: 0.0466\n",
      "Epoch 31/50\n",
      "21964/21964 [==============================] - 1s 55us/step - loss: 3.1792 - acc: 0.0443 - val_loss: 3.1784 - val_acc: 0.0441\n",
      "Epoch 32/50\n",
      "21964/21964 [==============================] - 1s 53us/step - loss: 3.1795 - acc: 0.0421 - val_loss: 3.1781 - val_acc: 0.0475\n",
      "Epoch 33/50\n",
      "21964/21964 [==============================] - 1s 53us/step - loss: 3.1792 - acc: 0.0454 - val_loss: 3.1748 - val_acc: 0.0475\n",
      "Epoch 34/50\n",
      "21964/21964 [==============================] - 1s 54us/step - loss: 3.1794 - acc: 0.0447 - val_loss: 3.1778 - val_acc: 0.0463\n",
      "Epoch 35/50\n",
      "21964/21964 [==============================] - 1s 52us/step - loss: 3.1790 - acc: 0.0451 - val_loss: 3.1787 - val_acc: 0.0413\n",
      "Epoch 36/50\n",
      "21964/21964 [==============================] - 1s 56us/step - loss: 3.1792 - acc: 0.0440 - val_loss: 3.1792 - val_acc: 0.0475\n",
      "Epoch 37/50\n",
      "21964/21964 [==============================] - 1s 53us/step - loss: 3.1794 - acc: 0.0433 - val_loss: 3.1776 - val_acc: 0.0494\n",
      "Epoch 38/50\n",
      "21964/21964 [==============================] - 1s 55us/step - loss: 3.1787 - acc: 0.0445 - val_loss: 3.1821 - val_acc: 0.0399\n",
      "Epoch 39/50\n",
      "21964/21964 [==============================] - 1s 54us/step - loss: 3.1790 - acc: 0.0422 - val_loss: 3.1767 - val_acc: 0.0530\n",
      "Epoch 40/50\n",
      "21964/21964 [==============================] - 1s 55us/step - loss: 3.1790 - acc: 0.0440 - val_loss: 3.1776 - val_acc: 0.0441\n",
      "Epoch 41/50\n",
      "21964/21964 [==============================] - 1s 56us/step - loss: 3.1789 - acc: 0.0443 - val_loss: 3.1760 - val_acc: 0.0475\n",
      "Epoch 42/50\n",
      "21964/21964 [==============================] - 1s 52us/step - loss: 3.1793 - acc: 0.0438 - val_loss: 3.1766 - val_acc: 0.0488\n",
      "Epoch 43/50\n",
      "21964/21964 [==============================] - 1s 53us/step - loss: 3.1791 - acc: 0.0436 - val_loss: 3.1785 - val_acc: 0.0419\n",
      "Epoch 44/50\n",
      "21964/21964 [==============================] - 1s 53us/step - loss: 3.1789 - acc: 0.0441 - val_loss: 3.1794 - val_acc: 0.0475\n",
      "Epoch 45/50\n",
      "21964/21964 [==============================] - 1s 53us/step - loss: 3.1789 - acc: 0.0458 - val_loss: 3.1783 - val_acc: 0.0530\n",
      "Epoch 46/50\n",
      "21964/21964 [==============================] - 1s 56us/step - loss: 3.1784 - acc: 0.0444 - val_loss: 3.1770 - val_acc: 0.0494\n",
      "Epoch 47/50\n",
      "21964/21964 [==============================] - 1s 56us/step - loss: 3.1792 - acc: 0.0456 - val_loss: 3.1793 - val_acc: 0.0475\n",
      "Epoch 48/50\n",
      "21964/21964 [==============================] - 1s 55us/step - loss: 3.1788 - acc: 0.0458 - val_loss: 3.1803 - val_acc: 0.0401\n",
      "Epoch 49/50\n",
      "21964/21964 [==============================] - 1s 58us/step - loss: 3.1792 - acc: 0.0432 - val_loss: 3.1784 - val_acc: 0.0399\n",
      "Epoch 50/50\n",
      "21964/21964 [==============================] - 1s 55us/step - loss: 3.1792 - acc: 0.0407 - val_loss: 3.1771 - val_acc: 0.0530\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2af8367d9b0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils.np_utils import to_categorical\n",
    "#Con la data sin pre-procesar una capa extra con tanh- ahora se usa elu por relu\n",
    "model2 = Sequential()\n",
    "model2.add(Dense(30, input_dim=x_tr.shape[1], init='uniform', activation='relu'))\n",
    "model2.add(Dense(30, init='uniform', activation='elu'))\n",
    "model2.add(Dense(30, init='uniform', activation='tanh'))\n",
    "model2.add(Dense(25, init='uniform', activation='softmax'))\n",
    "model2.compile(optimizer=SGD(lr=0.05), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model2.fit(x_tr.values, to_categorical(y_tr), nb_epoch=50, batch_size=128, verbose=1,\n",
    "                        validation_data=(x_v.values,to_categorical(y_v)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alfredo\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(30, input_dim=784, activation=\"relu\", kernel_initializer=\"uniform\")`\n",
      "  import sys\n",
      "C:\\Users\\Alfredo\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(30, activation=\"relu\", kernel_initializer=\"uniform\")`\n",
      "  \n",
      "C:\\Users\\Alfredo\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(30, activation=\"tanh\", kernel_initializer=\"uniform\")`\n",
      "  if __name__ == '__main__':\n",
      "C:\\Users\\Alfredo\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:10: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(25, activation=\"softmax\", kernel_initializer=\"uniform\")`\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "C:\\Users\\Alfredo\\Anaconda3\\lib\\site-packages\\keras\\models.py:939: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 21964 samples, validate on 5491 samples\n",
      "Epoch 1/50\n",
      "21964/21964 [==============================] - 2s 78us/step - loss: 3.2123 - acc: 0.0502 - val_loss: 3.2064 - val_acc: 0.0479\n",
      "Epoch 2/50\n",
      "21964/21964 [==============================] - 1s 50us/step - loss: 3.2027 - acc: 0.0471 - val_loss: 3.1989 - val_acc: 0.0486\n",
      "Epoch 3/50\n",
      "21964/21964 [==============================] - 1s 50us/step - loss: 3.1962 - acc: 0.0492 - val_loss: 3.1930 - val_acc: 0.0586\n",
      "Epoch 4/50\n",
      "21964/21964 [==============================] - 1s 50us/step - loss: 3.1880 - acc: 0.0601 - val_loss: 3.1777 - val_acc: 0.0688\n",
      "Epoch 5/50\n",
      "21964/21964 [==============================] - 1s 51us/step - loss: 3.1452 - acc: 0.0621 - val_loss: 3.1127 - val_acc: 0.0639\n",
      "Epoch 6/50\n",
      "21964/21964 [==============================] - 1s 51us/step - loss: 3.0648 - acc: 0.0891 - val_loss: 3.0004 - val_acc: 0.0894\n",
      "Epoch 7/50\n",
      "21964/21964 [==============================] - 1s 51us/step - loss: 2.8548 - acc: 0.1054 - val_loss: 2.7215 - val_acc: 0.1074\n",
      "Epoch 8/50\n",
      "21964/21964 [==============================] - 1s 50us/step - loss: 2.6018 - acc: 0.1442 - val_loss: 2.5028 - val_acc: 0.1599\n",
      "Epoch 9/50\n",
      "21964/21964 [==============================] - 1s 51us/step - loss: 2.3737 - acc: 0.2101 - val_loss: 2.2489 - val_acc: 0.2588\n",
      "Epoch 10/50\n",
      "21964/21964 [==============================] - 1s 51us/step - loss: 2.0888 - acc: 0.3152 - val_loss: 1.9211 - val_acc: 0.3642\n",
      "Epoch 11/50\n",
      "21964/21964 [==============================] - 1s 51us/step - loss: 1.7557 - acc: 0.4291 - val_loss: 1.5734 - val_acc: 0.4766\n",
      "Epoch 12/50\n",
      "21964/21964 [==============================] - 1s 52us/step - loss: 1.4660 - acc: 0.5342 - val_loss: 1.6818 - val_acc: 0.4451\n",
      "Epoch 13/50\n",
      "21964/21964 [==============================] - 1s 51us/step - loss: 1.2742 - acc: 0.6096 - val_loss: 1.1306 - val_acc: 0.6664\n",
      "Epoch 14/50\n",
      "21964/21964 [==============================] - 1s 52us/step - loss: 1.0652 - acc: 0.6881 - val_loss: 1.2910 - val_acc: 0.5910\n",
      "Epoch 15/50\n",
      "21964/21964 [==============================] - 1s 52us/step - loss: 0.8957 - acc: 0.7434 - val_loss: 0.8633 - val_acc: 0.7427\n",
      "Epoch 16/50\n",
      "21964/21964 [==============================] - 1s 52us/step - loss: 0.7577 - acc: 0.7893 - val_loss: 0.9311 - val_acc: 0.6948\n",
      "Epoch 17/50\n",
      "21964/21964 [==============================] - 1s 50us/step - loss: 0.6191 - acc: 0.8321 - val_loss: 0.5203 - val_acc: 0.8678\n",
      "Epoch 18/50\n",
      "21964/21964 [==============================] - 1s 52us/step - loss: 0.5121 - acc: 0.8639 - val_loss: 0.4423 - val_acc: 0.8931\n",
      "Epoch 19/50\n",
      "21964/21964 [==============================] - 1s 52us/step - loss: 0.4522 - acc: 0.8805 - val_loss: 0.3488 - val_acc: 0.9188\n",
      "Epoch 20/50\n",
      "21964/21964 [==============================] - 1s 54us/step - loss: 0.3496 - acc: 0.9174 - val_loss: 0.2883 - val_acc: 0.9408\n",
      "Epoch 21/50\n",
      "21964/21964 [==============================] - 1s 54us/step - loss: 0.3333 - acc: 0.9231 - val_loss: 0.2885 - val_acc: 0.9346\n",
      "Epoch 22/50\n",
      "21964/21964 [==============================] - 1s 53us/step - loss: 0.2711 - acc: 0.9420 - val_loss: 1.5022 - val_acc: 0.6245\n",
      "Epoch 23/50\n",
      "21964/21964 [==============================] - 1s 52us/step - loss: 0.2105 - acc: 0.9607 - val_loss: 0.7072 - val_acc: 0.7993\n",
      "Epoch 24/50\n",
      "21964/21964 [==============================] - 1s 51us/step - loss: 0.2308 - acc: 0.9502 - val_loss: 0.1658 - val_acc: 0.9718\n",
      "Epoch 25/50\n",
      "21964/21964 [==============================] - 1s 52us/step - loss: 0.2354 - acc: 0.9469 - val_loss: 0.1570 - val_acc: 0.9725\n",
      "Epoch 26/50\n",
      "21964/21964 [==============================] - 1s 51us/step - loss: 0.1967 - acc: 0.9581 - val_loss: 0.1327 - val_acc: 0.9771\n",
      "Epoch 27/50\n",
      "21964/21964 [==============================] - 1s 54us/step - loss: 0.1285 - acc: 0.9763 - val_loss: 0.1189 - val_acc: 0.9771\n",
      "Epoch 28/50\n",
      "21964/21964 [==============================] - 1s 58us/step - loss: 0.1113 - acc: 0.9786 - val_loss: 0.1084 - val_acc: 0.9780\n",
      "Epoch 29/50\n",
      "21964/21964 [==============================] - 1s 56us/step - loss: 0.1107 - acc: 0.9774 - val_loss: 0.0999 - val_acc: 0.9794\n",
      "Epoch 30/50\n",
      "21964/21964 [==============================] - 1s 52us/step - loss: 0.0907 - acc: 0.9830 - val_loss: 0.0948 - val_acc: 0.9807\n",
      "Epoch 31/50\n",
      "21964/21964 [==============================] - 1s 53us/step - loss: 0.1227 - acc: 0.9729 - val_loss: 0.0849 - val_acc: 0.9823\n",
      "Epoch 32/50\n",
      "21964/21964 [==============================] - 1s 53us/step - loss: 0.0741 - acc: 0.9848 - val_loss: 0.5825 - val_acc: 0.8355\n",
      "Epoch 33/50\n",
      "21964/21964 [==============================] - 1s 53us/step - loss: 0.1055 - acc: 0.9756 - val_loss: 0.0727 - val_acc: 0.9843\n",
      "Epoch 34/50\n",
      "21964/21964 [==============================] - 1s 53us/step - loss: 0.0634 - acc: 0.9893 - val_loss: 0.0670 - val_acc: 0.9885\n",
      "Epoch 35/50\n",
      "21964/21964 [==============================] - 1s 53us/step - loss: 0.0587 - acc: 0.9919 - val_loss: 0.0646 - val_acc: 0.9894\n",
      "Epoch 36/50\n",
      "21964/21964 [==============================] - 1s 55us/step - loss: 0.0547 - acc: 0.9933 - val_loss: 0.0588 - val_acc: 0.9913\n",
      "Epoch 37/50\n",
      "21964/21964 [==============================] - 1s 52us/step - loss: 0.0510 - acc: 0.9933 - val_loss: 0.0549 - val_acc: 0.9914\n",
      "Epoch 38/50\n",
      "21964/21964 [==============================] - 1s 52us/step - loss: 0.0538 - acc: 0.9916 - val_loss: 0.0531 - val_acc: 0.9914\n",
      "Epoch 39/50\n",
      "21964/21964 [==============================] - 1s 53us/step - loss: 0.1434 - acc: 0.9670 - val_loss: 0.1551 - val_acc: 0.9572\n",
      "Epoch 40/50\n",
      "21964/21964 [==============================] - 1s 52us/step - loss: 0.0497 - acc: 0.9927 - val_loss: 0.0487 - val_acc: 0.9918\n",
      "Epoch 41/50\n",
      "21964/21964 [==============================] - 1s 54us/step - loss: 0.0421 - acc: 0.9933 - val_loss: 0.0457 - val_acc: 0.9927\n",
      "Epoch 42/50\n",
      "21964/21964 [==============================] - 1s 53us/step - loss: 0.0395 - acc: 0.9947 - val_loss: 0.0436 - val_acc: 0.9936\n",
      "Epoch 43/50\n",
      "21964/21964 [==============================] - 1s 53us/step - loss: 0.0375 - acc: 0.9955 - val_loss: 0.0411 - val_acc: 0.9954\n",
      "Epoch 44/50\n",
      "21964/21964 [==============================] - 1s 53us/step - loss: 0.0354 - acc: 0.9965 - val_loss: 0.0395 - val_acc: 0.9956\n",
      "Epoch 45/50\n",
      "21964/21964 [==============================] - 1s 52us/step - loss: 0.0338 - acc: 0.9971 - val_loss: 0.0377 - val_acc: 0.9960\n",
      "Epoch 46/50\n",
      "21964/21964 [==============================] - 1s 55us/step - loss: 0.0324 - acc: 0.9972 - val_loss: 0.0379 - val_acc: 0.9953\n",
      "Epoch 47/50\n",
      "21964/21964 [==============================] - 1s 57us/step - loss: 0.0310 - acc: 0.9976 - val_loss: 0.0349 - val_acc: 0.9956\n",
      "Epoch 48/50\n",
      "21964/21964 [==============================] - 1s 56us/step - loss: 0.0302 - acc: 0.9975 - val_loss: 0.0361 - val_acc: 0.9958\n",
      "Epoch 49/50\n",
      "21964/21964 [==============================] - 1s 54us/step - loss: 0.0288 - acc: 0.9978 - val_loss: 0.0327 - val_acc: 0.9965\n",
      "Epoch 50/50\n",
      "21964/21964 [==============================] - 1s 55us/step - loss: 0.0276 - acc: 0.9979 - val_loss: 0.0312 - val_acc: 0.9965\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2af84817d30>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils.np_utils import to_categorical\n",
    "#Con la data pre-procesada una capa extra con tanh-\n",
    "model3 = Sequential()\n",
    "model3.add(Dense(30, input_dim=Sx_tr.shape[1], init='uniform', activation='relu'))\n",
    "model3.add(Dense(30, init='uniform', activation='relu'))\n",
    "model3.add(Dense(30, init='uniform', activation='tanh'))\n",
    "model3.add(Dense(25, init='uniform', activation='softmax'))\n",
    "model3.compile(optimizer=SGD(lr=0.05), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model3.fit(Sx_tr, to_categorical(y_tr), nb_epoch=50, batch_size=128, verbose=1,\n",
    "                    validation_data=(Sx_v,to_categorical(y_v)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alfredo\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(30, input_dim=784, activation=\"relu\", kernel_initializer=\"uniform\")`\n",
      "  import sys\n",
      "C:\\Users\\Alfredo\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(30, activation=\"elu\", kernel_initializer=\"uniform\")`\n",
      "  \n",
      "C:\\Users\\Alfredo\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(25, activation=\"softmax\", kernel_initializer=\"uniform\")`\n",
      "  if __name__ == '__main__':\n",
      "C:\\Users\\Alfredo\\Anaconda3\\lib\\site-packages\\keras\\models.py:939: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 21964 samples, validate on 5491 samples\n",
      "Epoch 1/50\n",
      "21964/21964 [==============================] - 2s 74us/step - loss: 3.1812 - acc: 0.1058 - val_loss: 3.0777 - val_acc: 0.1109\n",
      "Epoch 2/50\n",
      "21964/21964 [==============================] - 1s 53us/step - loss: 2.6966 - acc: 0.1799 - val_loss: 2.2330 - val_acc: 0.2879\n",
      "Epoch 3/50\n",
      "21964/21964 [==============================] - 1s 52us/step - loss: 1.8607 - acc: 0.3869 - val_loss: 1.5684 - val_acc: 0.4821\n",
      "Epoch 4/50\n",
      "21964/21964 [==============================] - 1s 53us/step - loss: 1.3030 - acc: 0.5651 - val_loss: 1.1099 - val_acc: 0.6217\n",
      "Epoch 5/50\n",
      "21964/21964 [==============================] - 1s 52us/step - loss: 0.9110 - acc: 0.6908 - val_loss: 0.7767 - val_acc: 0.7385\n",
      "Epoch 6/50\n",
      "21964/21964 [==============================] - 1s 54us/step - loss: 0.6429 - acc: 0.7971 - val_loss: 0.5532 - val_acc: 0.8312\n",
      "Epoch 7/50\n",
      "21964/21964 [==============================] - 1s 52us/step - loss: 0.4528 - acc: 0.8669 - val_loss: 0.4136 - val_acc: 0.8694\n",
      "Epoch 8/50\n",
      "21964/21964 [==============================] - 1s 53us/step - loss: 0.3145 - acc: 0.9199 - val_loss: 0.2848 - val_acc: 0.9195\n",
      "Epoch 9/50\n",
      "21964/21964 [==============================] - 1s 52us/step - loss: 0.2115 - acc: 0.9564 - val_loss: 0.1874 - val_acc: 0.9656\n",
      "Epoch 10/50\n",
      "21964/21964 [==============================] - 1s 54us/step - loss: 0.1414 - acc: 0.9806 - val_loss: 0.1242 - val_acc: 0.9842\n",
      "Epoch 11/50\n",
      "21964/21964 [==============================] - 1s 55us/step - loss: 0.0958 - acc: 0.9921 - val_loss: 0.0837 - val_acc: 0.9945\n",
      "Epoch 12/50\n",
      "21964/21964 [==============================] - 1s 54us/step - loss: 0.0659 - acc: 0.9980 - val_loss: 0.0608 - val_acc: 0.9985\n",
      "Epoch 13/50\n",
      "21964/21964 [==============================] - 1s 52us/step - loss: 0.0475 - acc: 0.9994 - val_loss: 0.0445 - val_acc: 0.9985\n",
      "Epoch 14/50\n",
      "21964/21964 [==============================] - 1s 52us/step - loss: 0.0361 - acc: 0.9996 - val_loss: 0.0344 - val_acc: 0.9991\n",
      "Epoch 15/50\n",
      "21964/21964 [==============================] - 1s 55us/step - loss: 0.0284 - acc: 0.9998 - val_loss: 0.0278 - val_acc: 0.9993\n",
      "Epoch 16/50\n",
      "21964/21964 [==============================] - 1s 53us/step - loss: 0.0232 - acc: 0.9997 - val_loss: 0.0232 - val_acc: 0.9996\n",
      "Epoch 17/50\n",
      "21964/21964 [==============================] - 1s 56us/step - loss: 0.0192 - acc: 0.9999 - val_loss: 0.0200 - val_acc: 0.9995\n",
      "Epoch 18/50\n",
      "21964/21964 [==============================] - 1s 54us/step - loss: 0.0163 - acc: 0.9998 - val_loss: 0.0171 - val_acc: 0.9995\n",
      "Epoch 19/50\n",
      "21964/21964 [==============================] - 1s 55us/step - loss: 0.0141 - acc: 0.9999 - val_loss: 0.0150 - val_acc: 0.9995\n",
      "Epoch 20/50\n",
      "21964/21964 [==============================] - 1s 56us/step - loss: 0.0124 - acc: 0.9999 - val_loss: 0.0132 - val_acc: 0.9998\n",
      "Epoch 21/50\n",
      "21964/21964 [==============================] - 1s 55us/step - loss: 0.0110 - acc: 1.0000 - val_loss: 0.0120 - val_acc: 0.9998\n",
      "Epoch 22/50\n",
      "21964/21964 [==============================] - 1s 53us/step - loss: 0.0099 - acc: 1.0000 - val_loss: 0.0108 - val_acc: 0.9998\n",
      "Epoch 23/50\n",
      "21964/21964 [==============================] - 1s 54us/step - loss: 0.0089 - acc: 1.0000 - val_loss: 0.0099 - val_acc: 0.9996\n",
      "Epoch 24/50\n",
      "21964/21964 [==============================] - 1s 54us/step - loss: 0.0081 - acc: 1.0000 - val_loss: 0.0091 - val_acc: 0.9998\n",
      "Epoch 25/50\n",
      "21964/21964 [==============================] - 1s 53us/step - loss: 0.0074 - acc: 1.0000 - val_loss: 0.0085 - val_acc: 0.9998\n",
      "Epoch 26/50\n",
      "21964/21964 [==============================] - 1s 56us/step - loss: 0.0068 - acc: 1.0000 - val_loss: 0.0079 - val_acc: 0.9996\n",
      "Epoch 27/50\n",
      "21964/21964 [==============================] - 1s 57us/step - loss: 0.0064 - acc: 1.0000 - val_loss: 0.0073 - val_acc: 0.9996\n",
      "Epoch 28/50\n",
      "21964/21964 [==============================] - 1s 61us/step - loss: 0.0059 - acc: 1.0000 - val_loss: 0.0070 - val_acc: 0.9998\n",
      "Epoch 29/50\n",
      "21964/21964 [==============================] - 1s 52us/step - loss: 0.0055 - acc: 1.0000 - val_loss: 0.0065 - val_acc: 0.9996\n",
      "Epoch 30/50\n",
      "21964/21964 [==============================] - 1s 50us/step - loss: 0.0052 - acc: 1.0000 - val_loss: 0.0061 - val_acc: 0.9998\n",
      "Epoch 31/50\n",
      "21964/21964 [==============================] - 1s 52us/step - loss: 0.0048 - acc: 1.0000 - val_loss: 0.0058 - val_acc: 0.9998\n",
      "Epoch 32/50\n",
      "21964/21964 [==============================] - 1s 52us/step - loss: 0.0046 - acc: 1.0000 - val_loss: 0.0057 - val_acc: 0.9998\n",
      "Epoch 33/50\n",
      "21964/21964 [==============================] - 1s 50us/step - loss: 0.0043 - acc: 1.0000 - val_loss: 0.0053 - val_acc: 0.9998\n",
      "Epoch 34/50\n",
      "21964/21964 [==============================] - 1s 51us/step - loss: 0.0041 - acc: 1.0000 - val_loss: 0.0051 - val_acc: 0.9998\n",
      "Epoch 35/50\n",
      "21964/21964 [==============================] - 1s 50us/step - loss: 0.0039 - acc: 1.0000 - val_loss: 0.0049 - val_acc: 0.9998\n",
      "Epoch 36/50\n",
      "21964/21964 [==============================] - 1s 51us/step - loss: 0.0037 - acc: 1.0000 - val_loss: 0.0046 - val_acc: 0.9998\n",
      "Epoch 37/50\n",
      "21964/21964 [==============================] - 1s 50us/step - loss: 0.0035 - acc: 1.0000 - val_loss: 0.0046 - val_acc: 0.9998\n",
      "Epoch 38/50\n",
      "21964/21964 [==============================] - 1s 50us/step - loss: 0.0034 - acc: 1.0000 - val_loss: 0.0044 - val_acc: 0.9998\n",
      "Epoch 39/50\n",
      "21964/21964 [==============================] - 1s 50us/step - loss: 0.0032 - acc: 1.0000 - val_loss: 0.0042 - val_acc: 0.9998\n",
      "Epoch 40/50\n",
      "21964/21964 [==============================] - 1s 49us/step - loss: 0.0031 - acc: 1.0000 - val_loss: 0.0041 - val_acc: 0.9996\n",
      "Epoch 41/50\n",
      "21964/21964 [==============================] - 1s 50us/step - loss: 0.0030 - acc: 1.0000 - val_loss: 0.0039 - val_acc: 0.9998\n",
      "Epoch 42/50\n",
      "21964/21964 [==============================] - 1s 50us/step - loss: 0.0028 - acc: 1.0000 - val_loss: 0.0038 - val_acc: 0.9998\n",
      "Epoch 43/50\n",
      "21964/21964 [==============================] - 1s 50us/step - loss: 0.0027 - acc: 1.0000 - val_loss: 0.0036 - val_acc: 0.9998\n",
      "Epoch 44/50\n",
      "21964/21964 [==============================] - 1s 50us/step - loss: 0.0026 - acc: 1.0000 - val_loss: 0.0035 - val_acc: 0.9998\n",
      "Epoch 45/50\n",
      "21964/21964 [==============================] - 1s 55us/step - loss: 0.0025 - acc: 1.0000 - val_loss: 0.0034 - val_acc: 0.9998\n",
      "Epoch 46/50\n",
      "21964/21964 [==============================] - 1s 53us/step - loss: 0.0024 - acc: 1.0000 - val_loss: 0.0034 - val_acc: 0.9998\n",
      "Epoch 47/50\n",
      "21964/21964 [==============================] - 1s 49us/step - loss: 0.0024 - acc: 1.0000 - val_loss: 0.0033 - val_acc: 0.9998\n",
      "Epoch 48/50\n",
      "21964/21964 [==============================] - 1s 50us/step - loss: 0.0023 - acc: 1.0000 - val_loss: 0.0032 - val_acc: 0.9998\n",
      "Epoch 49/50\n",
      "21964/21964 [==============================] - 1s 51us/step - loss: 0.0022 - acc: 1.0000 - val_loss: 0.0031 - val_acc: 0.9998\n",
      "Epoch 50/50\n",
      "21964/21964 [==============================] - 1s 52us/step - loss: 0.0021 - acc: 1.0000 - val_loss: 0.0030 - val_acc: 0.9998\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2af8329a320>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils.np_utils import to_categorical\n",
    "#Con la data pre-procesada una capa menos a la anterior y una capa ahora usa elu en vez de relu\n",
    "model4 = Sequential()\n",
    "model4.add(Dense(30, input_dim=Sx_tr.shape[1], init='uniform', activation='relu'))\n",
    "model4.add(Dense(30, init='uniform', activation='elu'))\n",
    "model4.add(Dense(25, init='uniform', activation='softmax'))\n",
    "model4.compile(optimizer=SGD(lr=0.05), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model4.fit(Sx_tr, to_categorical(y_tr), nb_epoch=50, batch_size=128, verbose=1,\n",
    "                    validation_data=(Sx_v,to_categorical(y_v)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alfredo\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(30, input_dim=784, activation=\"linear\", kernel_initializer=\"uniform\")`\n",
      "  import sys\n",
      "C:\\Users\\Alfredo\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(30, activation=\"elu\", kernel_initializer=\"uniform\")`\n",
      "  \n",
      "C:\\Users\\Alfredo\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(25, activation=\"softmax\", kernel_initializer=\"uniform\")`\n",
      "  if __name__ == '__main__':\n",
      "C:\\Users\\Alfredo\\Anaconda3\\lib\\site-packages\\keras\\models.py:939: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 21964 samples, validate on 5491 samples\n",
      "Epoch 1/50\n",
      "21964/21964 [==============================] - 2s 74us/step - loss: 2.9809 - acc: 0.1403 - val_loss: 2.5406 - val_acc: 0.2001\n",
      "Epoch 2/50\n",
      "21964/21964 [==============================] - 1s 55us/step - loss: 2.0246 - acc: 0.3442 - val_loss: 1.6692 - val_acc: 0.4409\n",
      "Epoch 3/50\n",
      "21964/21964 [==============================] - 1s 53us/step - loss: 1.4119 - acc: 0.5270 - val_loss: 1.2359 - val_acc: 0.5830\n",
      "Epoch 4/50\n",
      "21964/21964 [==============================] - 1s 54us/step - loss: 1.0372 - acc: 0.6475 - val_loss: 0.9296 - val_acc: 0.6724\n",
      "Epoch 5/50\n",
      "21964/21964 [==============================] - 1s 54us/step - loss: 0.7752 - acc: 0.7468 - val_loss: 0.6941 - val_acc: 0.7705\n",
      "Epoch 6/50\n",
      "21964/21964 [==============================] - 1s 57us/step - loss: 0.5764 - acc: 0.8217 - val_loss: 0.5204 - val_acc: 0.8403\n",
      "Epoch 7/50\n",
      "21964/21964 [==============================] - 1s 54us/step - loss: 0.4240 - acc: 0.8776 - val_loss: 0.3819 - val_acc: 0.8913\n",
      "Epoch 8/50\n",
      "21964/21964 [==============================] - 1s 54us/step - loss: 0.3091 - acc: 0.9227 - val_loss: 0.2968 - val_acc: 0.9164\n",
      "Epoch 9/50\n",
      "21964/21964 [==============================] - 1s 54us/step - loss: 0.2196 - acc: 0.9542 - val_loss: 0.1894 - val_acc: 0.9638\n",
      "Epoch 10/50\n",
      "21964/21964 [==============================] - 1s 54us/step - loss: 0.1507 - acc: 0.9788 - val_loss: 0.1366 - val_acc: 0.9823\n",
      "Epoch 11/50\n",
      "21964/21964 [==============================] - 1s 54us/step - loss: 0.1066 - acc: 0.9893 - val_loss: 0.0927 - val_acc: 0.9931\n",
      "Epoch 12/50\n",
      "21964/21964 [==============================] - 1s 54us/step - loss: 0.0745 - acc: 0.9958 - val_loss: 0.0709 - val_acc: 0.9980\n",
      "Epoch 13/50\n",
      "21964/21964 [==============================] - 1s 54us/step - loss: 0.0556 - acc: 0.9978 - val_loss: 0.0545 - val_acc: 0.9976\n",
      "Epoch 14/50\n",
      "21964/21964 [==============================] - 1s 55us/step - loss: 0.0426 - acc: 0.9989 - val_loss: 0.0414 - val_acc: 0.9987\n",
      "Epoch 15/50\n",
      "21964/21964 [==============================] - 1s 56us/step - loss: 0.0341 - acc: 0.9991 - val_loss: 0.0331 - val_acc: 0.9991\n",
      "Epoch 16/50\n",
      "21964/21964 [==============================] - 1s 55us/step - loss: 0.0279 - acc: 0.9997 - val_loss: 0.0280 - val_acc: 0.9993\n",
      "Epoch 17/50\n",
      "21964/21964 [==============================] - 1s 56us/step - loss: 0.0238 - acc: 0.9996 - val_loss: 0.0240 - val_acc: 0.9993\n",
      "Epoch 18/50\n",
      "21964/21964 [==============================] - 1s 54us/step - loss: 0.0201 - acc: 0.9998 - val_loss: 0.0210 - val_acc: 0.9991\n",
      "Epoch 19/50\n",
      "21964/21964 [==============================] - 1s 56us/step - loss: 0.0174 - acc: 0.9998 - val_loss: 0.0185 - val_acc: 0.9993\n",
      "Epoch 20/50\n",
      "21964/21964 [==============================] - 1s 54us/step - loss: 0.0152 - acc: 0.9998 - val_loss: 0.0162 - val_acc: 0.9995\n",
      "Epoch 21/50\n",
      "21964/21964 [==============================] - 1s 57us/step - loss: 0.0137 - acc: 0.9998 - val_loss: 0.0150 - val_acc: 0.9995\n",
      "Epoch 22/50\n",
      "21964/21964 [==============================] - 1s 55us/step - loss: 0.0122 - acc: 0.9999 - val_loss: 0.0136 - val_acc: 0.9993\n",
      "Epoch 23/50\n",
      "21964/21964 [==============================] - 1s 55us/step - loss: 0.0110 - acc: 0.9999 - val_loss: 0.0124 - val_acc: 0.9995\n",
      "Epoch 24/50\n",
      "21964/21964 [==============================] - 1s 55us/step - loss: 0.0101 - acc: 0.9999 - val_loss: 0.0113 - val_acc: 0.9995\n",
      "Epoch 25/50\n",
      "21964/21964 [==============================] - 1s 57us/step - loss: 0.0092 - acc: 0.9999 - val_loss: 0.0106 - val_acc: 0.9995\n",
      "Epoch 26/50\n",
      "21964/21964 [==============================] - 1s 56us/step - loss: 0.0085 - acc: 1.0000 - val_loss: 0.0098 - val_acc: 0.9991\n",
      "Epoch 27/50\n",
      "21964/21964 [==============================] - 1s 56us/step - loss: 0.0079 - acc: 0.9999 - val_loss: 0.0091 - val_acc: 0.9995\n",
      "Epoch 28/50\n",
      "21964/21964 [==============================] - 1s 58us/step - loss: 0.0073 - acc: 1.0000 - val_loss: 0.0086 - val_acc: 0.9995\n",
      "Epoch 29/50\n",
      "21964/21964 [==============================] - 1s 57us/step - loss: 0.0068 - acc: 1.0000 - val_loss: 0.0080 - val_acc: 0.9995\n",
      "Epoch 30/50\n",
      "21964/21964 [==============================] - 1s 56us/step - loss: 0.0064 - acc: 1.0000 - val_loss: 0.0076 - val_acc: 0.9995\n",
      "Epoch 31/50\n",
      "21964/21964 [==============================] - 1s 56us/step - loss: 0.0060 - acc: 1.0000 - val_loss: 0.0073 - val_acc: 0.9995\n",
      "Epoch 32/50\n",
      "21964/21964 [==============================] - 1s 56us/step - loss: 0.0056 - acc: 1.0000 - val_loss: 0.0069 - val_acc: 0.9995\n",
      "Epoch 33/50\n",
      "21964/21964 [==============================] - 1s 58us/step - loss: 0.0053 - acc: 1.0000 - val_loss: 0.0067 - val_acc: 0.9995\n",
      "Epoch 34/50\n",
      "21964/21964 [==============================] - 1s 57us/step - loss: 0.0051 - acc: 1.0000 - val_loss: 0.0064 - val_acc: 0.9995\n",
      "Epoch 35/50\n",
      "21964/21964 [==============================] - 1s 58us/step - loss: 0.0048 - acc: 1.0000 - val_loss: 0.0061 - val_acc: 0.9995\n",
      "Epoch 36/50\n",
      "21964/21964 [==============================] - 1s 60us/step - loss: 0.0046 - acc: 1.0000 - val_loss: 0.0058 - val_acc: 0.9995\n",
      "Epoch 37/50\n",
      "21964/21964 [==============================] - 1s 60us/step - loss: 0.0044 - acc: 1.0000 - val_loss: 0.0056 - val_acc: 0.9995\n",
      "Epoch 38/50\n",
      "21964/21964 [==============================] - 1s 60us/step - loss: 0.0041 - acc: 1.0000 - val_loss: 0.0054 - val_acc: 0.9995\n",
      "Epoch 39/50\n",
      "21964/21964 [==============================] - 1s 58us/step - loss: 0.0040 - acc: 1.0000 - val_loss: 0.0052 - val_acc: 0.9995\n",
      "Epoch 40/50\n",
      "21964/21964 [==============================] - 1s 59us/step - loss: 0.0038 - acc: 1.0000 - val_loss: 0.0051 - val_acc: 0.9995\n",
      "Epoch 41/50\n",
      "21964/21964 [==============================] - 1s 60us/step - loss: 0.0036 - acc: 1.0000 - val_loss: 0.0048 - val_acc: 0.9995\n",
      "Epoch 42/50\n",
      "21964/21964 [==============================] - 1s 59us/step - loss: 0.0035 - acc: 1.0000 - val_loss: 0.0047 - val_acc: 0.9995\n",
      "Epoch 43/50\n",
      "21964/21964 [==============================] - 1s 58us/step - loss: 0.0034 - acc: 1.0000 - val_loss: 0.0046 - val_acc: 0.9995\n",
      "Epoch 44/50\n",
      "21964/21964 [==============================] - 1s 60us/step - loss: 0.0032 - acc: 1.0000 - val_loss: 0.0045 - val_acc: 0.9995\n",
      "Epoch 45/50\n",
      "21964/21964 [==============================] - 1s 58us/step - loss: 0.0031 - acc: 1.0000 - val_loss: 0.0043 - val_acc: 0.9995\n",
      "Epoch 46/50\n",
      "21964/21964 [==============================] - 1s 59us/step - loss: 0.0030 - acc: 1.0000 - val_loss: 0.0042 - val_acc: 0.9995\n",
      "Epoch 47/50\n",
      "21964/21964 [==============================] - 1s 60us/step - loss: 0.0029 - acc: 1.0000 - val_loss: 0.0041 - val_acc: 0.9995\n",
      "Epoch 48/50\n",
      "21964/21964 [==============================] - 1s 62us/step - loss: 0.0028 - acc: 1.0000 - val_loss: 0.0040 - val_acc: 0.9995\n",
      "Epoch 49/50\n",
      "21964/21964 [==============================] - 1s 59us/step - loss: 0.0027 - acc: 1.0000 - val_loss: 0.0038 - val_acc: 0.9995\n",
      "Epoch 50/50\n",
      "21964/21964 [==============================] - 1s 59us/step - loss: 0.0026 - acc: 1.0000 - val_loss: 0.0038 - val_acc: 0.9996\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2af82e94ac8>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils.np_utils import to_categorical\n",
    "#Con la data pre-procesada capa inicial con activacion lineal\n",
    "########Con LINEAL al final no aprende :O####################\n",
    "model5 = Sequential()\n",
    "model5.add(Dense(30, input_dim=Sx_tr.shape[1], init='uniform', activation='linear'))\n",
    "model5.add(Dense(30, init='uniform', activation='elu'))\n",
    "model5.add(Dense(25, init='uniform', activation='softmax'))\n",
    "model5.compile(optimizer=SGD(lr=0.05), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model5.fit(Sx_tr, to_categorical(y_tr), nb_epoch=50, batch_size=128, verbose=1,\n",
    "                    validation_data=(Sx_v,to_categorical(y_v)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alfredo\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(30, input_dim=784, activation=\"relu\", kernel_initializer=\"uniform\")`\n",
      "  \n",
      "C:\\Users\\Alfredo\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(30, activation=\"elu\", kernel_initializer=\"uniform\")`\n",
      "  if __name__ == '__main__':\n",
      "C:\\Users\\Alfredo\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:10: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(25, activation=\"softmax\", kernel_initializer=\"uniform\")`\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "C:\\Users\\Alfredo\\Anaconda3\\lib\\site-packages\\keras\\models.py:939: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 21964 samples, validate on 5491 samples\n",
      "Epoch 1/50\n",
      "21964/21964 [==============================] - 2s 74us/step - loss: 3.1829 - acc: 0.1035 - val_loss: 3.0938 - val_acc: 0.1457\n",
      "Epoch 2/50\n",
      "21964/21964 [==============================] - 1s 57us/step - loss: 2.7606 - acc: 0.1698 - val_loss: 2.2827 - val_acc: 0.2657\n",
      "Epoch 3/50\n",
      "21964/21964 [==============================] - 1s 53us/step - loss: 1.8727 - acc: 0.3763 - val_loss: 1.5796 - val_acc: 0.4626\n",
      "Epoch 4/50\n",
      "21964/21964 [==============================] - 1s 52us/step - loss: 1.2998 - acc: 0.5705 - val_loss: 1.0871 - val_acc: 0.6323\n",
      "Epoch 5/50\n",
      "21964/21964 [==============================] - 1s 53us/step - loss: 0.8860 - acc: 0.7166 - val_loss: 0.7680 - val_acc: 0.7514\n",
      "Epoch 6/50\n",
      "21964/21964 [==============================] - 1s 55us/step - loss: 0.6157 - acc: 0.8134 - val_loss: 0.5432 - val_acc: 0.8419\n",
      "Epoch 7/50\n",
      "21964/21964 [==============================] - 1s 53us/step - loss: 0.4245 - acc: 0.8905 - val_loss: 0.3831 - val_acc: 0.9035\n",
      "Epoch 8/50\n",
      "21964/21964 [==============================] - 1s 52us/step - loss: 0.2906 - acc: 0.9337 - val_loss: 0.2626 - val_acc: 0.9383\n",
      "Epoch 9/50\n",
      "21964/21964 [==============================] - 1s 53us/step - loss: 0.1951 - acc: 0.9632 - val_loss: 0.1720 - val_acc: 0.9712\n",
      "Epoch 10/50\n",
      "21964/21964 [==============================] - 1s 52us/step - loss: 0.1278 - acc: 0.9840 - val_loss: 0.1156 - val_acc: 0.9852\n",
      "Epoch 11/50\n",
      "21964/21964 [==============================] - 1s 53us/step - loss: 0.0852 - acc: 0.9940 - val_loss: 0.0781 - val_acc: 0.9947\n",
      "Epoch 12/50\n",
      "21964/21964 [==============================] - 1s 52us/step - loss: 0.0592 - acc: 0.9979 - val_loss: 0.0550 - val_acc: 0.9985\n",
      "Epoch 13/50\n",
      "21964/21964 [==============================] - 1s 52us/step - loss: 0.0433 - acc: 0.9993 - val_loss: 0.0416 - val_acc: 0.9993\n",
      "Epoch 14/50\n",
      "21964/21964 [==============================] - 1s 52us/step - loss: 0.0330 - acc: 0.9997 - val_loss: 0.0330 - val_acc: 0.9996\n",
      "Epoch 15/50\n",
      "21964/21964 [==============================] - 1s 53us/step - loss: 0.0262 - acc: 0.9998 - val_loss: 0.0266 - val_acc: 0.9996\n",
      "Epoch 16/50\n",
      "21964/21964 [==============================] - 1s 53us/step - loss: 0.0214 - acc: 0.9999 - val_loss: 0.0224 - val_acc: 0.9996\n",
      "Epoch 17/50\n",
      "21964/21964 [==============================] - 1s 52us/step - loss: 0.0180 - acc: 1.0000 - val_loss: 0.0192 - val_acc: 0.9995\n",
      "Epoch 18/50\n",
      "21964/21964 [==============================] - 1s 53us/step - loss: 0.0153 - acc: 1.0000 - val_loss: 0.0165 - val_acc: 0.9995\n",
      "Epoch 19/50\n",
      "21964/21964 [==============================] - 1s 53us/step - loss: 0.0133 - acc: 1.0000 - val_loss: 0.0146 - val_acc: 0.9996\n",
      "Epoch 20/50\n",
      "21964/21964 [==============================] - 1s 51us/step - loss: 0.0117 - acc: 1.0000 - val_loss: 0.0129 - val_acc: 0.9996\n",
      "Epoch 21/50\n",
      "21964/21964 [==============================] - 1s 54us/step - loss: 0.0104 - acc: 1.0000 - val_loss: 0.0120 - val_acc: 0.9998\n",
      "Epoch 22/50\n",
      "21964/21964 [==============================] - 1s 52us/step - loss: 0.0094 - acc: 1.0000 - val_loss: 0.0107 - val_acc: 0.9996\n",
      "Epoch 23/50\n",
      "21964/21964 [==============================] - 1s 51us/step - loss: 0.0085 - acc: 1.0000 - val_loss: 0.0097 - val_acc: 0.9996\n",
      "Epoch 24/50\n",
      "21964/21964 [==============================] - 1s 53us/step - loss: 0.0077 - acc: 1.0000 - val_loss: 0.0090 - val_acc: 0.9996\n",
      "Epoch 25/50\n",
      "21964/21964 [==============================] - 1s 54us/step - loss: 0.0071 - acc: 1.0000 - val_loss: 0.0082 - val_acc: 0.9998\n",
      "Epoch 26/50\n",
      "21964/21964 [==============================] - 1s 52us/step - loss: 0.0065 - acc: 1.0000 - val_loss: 0.0077 - val_acc: 0.9996\n",
      "Epoch 27/50\n",
      "21964/21964 [==============================] - 1s 52us/step - loss: 0.0061 - acc: 1.0000 - val_loss: 0.0071 - val_acc: 0.9998\n",
      "Epoch 28/50\n",
      "21964/21964 [==============================] - 1s 55us/step - loss: 0.0056 - acc: 1.0000 - val_loss: 0.0067 - val_acc: 0.9998\n",
      "Epoch 29/50\n",
      "21964/21964 [==============================] - 1s 63us/step - loss: 0.0053 - acc: 1.0000 - val_loss: 0.0064 - val_acc: 1.0000\n",
      "Epoch 30/50\n",
      "21964/21964 [==============================] - 1s 68us/step - loss: 0.0049 - acc: 1.0000 - val_loss: 0.0060 - val_acc: 0.9998\n",
      "Epoch 31/50\n",
      "21964/21964 [==============================] - 1s 59us/step - loss: 0.0046 - acc: 1.0000 - val_loss: 0.0056 - val_acc: 0.9998\n",
      "Epoch 32/50\n",
      "21964/21964 [==============================] - 1s 54us/step - loss: 0.0044 - acc: 1.0000 - val_loss: 0.0054 - val_acc: 0.9998\n",
      "Epoch 33/50\n",
      "21964/21964 [==============================] - 1s 54us/step - loss: 0.0041 - acc: 1.0000 - val_loss: 0.0051 - val_acc: 0.9998\n",
      "Epoch 34/50\n",
      "21964/21964 [==============================] - 1s 56us/step - loss: 0.0039 - acc: 1.0000 - val_loss: 0.0049 - val_acc: 0.9998\n",
      "Epoch 35/50\n",
      "21964/21964 [==============================] - ETA: 0s - loss: 0.0037 - acc: 1.000 - 1s 55us/step - loss: 0.0037 - acc: 1.0000 - val_loss: 0.0047 - val_acc: 0.9998\n",
      "Epoch 36/50\n",
      "21964/21964 [==============================] - 1s 59us/step - loss: 0.0035 - acc: 1.0000 - val_loss: 0.0045 - val_acc: 0.9998\n",
      "Epoch 37/50\n",
      "21964/21964 [==============================] - 1s 52us/step - loss: 0.0034 - acc: 1.0000 - val_loss: 0.0043 - val_acc: 0.9998\n",
      "Epoch 38/50\n",
      "21964/21964 [==============================] - 1s 56us/step - loss: 0.0032 - acc: 1.0000 - val_loss: 0.0042 - val_acc: 1.0000\n",
      "Epoch 39/50\n",
      "21964/21964 [==============================] - 2s 72us/step - loss: 0.0031 - acc: 1.0000 - val_loss: 0.0040 - val_acc: 0.9998\n",
      "Epoch 40/50\n",
      "21964/21964 [==============================] - 1s 63us/step - loss: 0.0029 - acc: 1.0000 - val_loss: 0.0038 - val_acc: 0.9998\n",
      "Epoch 41/50\n",
      "21964/21964 [==============================] - 1s 64us/step - loss: 0.0028 - acc: 1.0000 - val_loss: 0.0037 - val_acc: 0.9998\n",
      "Epoch 42/50\n",
      "21964/21964 [==============================] - 1s 54us/step - loss: 0.0027 - acc: 1.0000 - val_loss: 0.0036 - val_acc: 0.9998\n",
      "Epoch 43/50\n",
      "21964/21964 [==============================] - 2s 69us/step - loss: 0.0026 - acc: 1.0000 - val_loss: 0.0035 - val_acc: 1.0000\n",
      "Epoch 44/50\n",
      "21964/21964 [==============================] - 2s 75us/step - loss: 0.0025 - acc: 1.0000 - val_loss: 0.0033 - val_acc: 0.9998\n",
      "Epoch 45/50\n",
      "21964/21964 [==============================] - 1s 64us/step - loss: 0.0024 - acc: 1.0000 - val_loss: 0.0032 - val_acc: 0.9998\n",
      "Epoch 46/50\n",
      "21964/21964 [==============================] - 2s 75us/step - loss: 0.0023 - acc: 1.0000 - val_loss: 0.0031 - val_acc: 0.9998\n",
      "Epoch 47/50\n",
      "21964/21964 [==============================] - 1s 66us/step - loss: 0.0023 - acc: 1.0000 - val_loss: 0.0031 - val_acc: 1.0000\n",
      "Epoch 48/50\n",
      "21964/21964 [==============================] - 1s 57us/step - loss: 0.0022 - acc: 1.0000 - val_loss: 0.0030 - val_acc: 0.9998\n",
      "Epoch 49/50\n",
      "21964/21964 [==============================] - 1s 53us/step - loss: 0.0021 - acc: 1.0000 - val_loss: 0.0029 - val_acc: 0.9998\n",
      "Epoch 50/50\n",
      "21964/21964 [==============================] - 1s 56us/step - loss: 0.0020 - acc: 1.0000 - val_loss: 0.0028 - val_acc: 0.9998\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2af852a9a90>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S2x_tr,S2x_v,S2x_t = escalador(x_tr,x_v,x_t,n=100.0)#con n 100, no tiene mucha diferencia con hacer n 255\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils.np_utils import to_categorical\n",
    "#Con la data pre-procesada una capa menos a la anterior y una capa ahora usa elu en vez de relu\n",
    "model6 = Sequential()\n",
    "model6.add(Dense(30, input_dim=S2x_tr.shape[1], init='uniform', activation='relu'))\n",
    "model6.add(Dense(30, init='uniform', activation='elu'))\n",
    "model6.add(Dense(25, init='uniform', activation='softmax'))\n",
    "model6.compile(optimizer=SGD(lr=0.05), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model6.fit(S2x_tr, to_categorical(y_tr), nb_epoch=50, batch_size=128, verbose=1,\n",
    "                    validation_data=(S2x_v,to_categorical(y_v)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alfredo\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(30, input_dim=784, activation=\"relu\", kernel_initializer=\"uniform\")`\n",
      "  import sys\n",
      "C:\\Users\\Alfredo\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(30, activation=\"elu\", kernel_initializer=\"uniform\")`\n",
      "  \n",
      "C:\\Users\\Alfredo\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(25, activation=\"softmax\", kernel_initializer=\"uniform\")`\n",
      "  if __name__ == '__main__':\n",
      "C:\\Users\\Alfredo\\Anaconda3\\lib\\site-packages\\keras\\models.py:939: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 21964 samples, validate on 5491 samples\n",
      "Epoch 1/150\n",
      "21964/21964 [==============================] - 2s 78us/step - loss: 1.0008 - acc: 0.0342 - val_loss: 1.0007 - val_acc: 0.0412\n",
      "Epoch 2/150\n",
      "21964/21964 [==============================] - 1s 50us/step - loss: 1.0007 - acc: 0.0387 - val_loss: 1.0006 - val_acc: 0.0450\n",
      "Epoch 3/150\n",
      "21964/21964 [==============================] - 1s 51us/step - loss: 1.0006 - acc: 0.0481 - val_loss: 1.0005 - val_acc: 0.0534\n",
      "Epoch 4/150\n",
      "21964/21964 [==============================] - 1s 59us/step - loss: 1.0005 - acc: 0.0577 - val_loss: 1.0005 - val_acc: 0.0634\n",
      "Epoch 5/150\n",
      "21964/21964 [==============================] - 1s 51us/step - loss: 1.0004 - acc: 0.0690 - val_loss: 1.0004 - val_acc: 0.0745\n",
      "Epoch 6/150\n",
      "21964/21964 [==============================] - 1s 49us/step - loss: 1.0004 - acc: 0.0821 - val_loss: 1.0004 - val_acc: 0.0887\n",
      "Epoch 7/150\n",
      "21964/21964 [==============================] - 1s 51us/step - loss: 1.0004 - acc: 0.0963 - val_loss: 1.0003 - val_acc: 0.1020\n",
      "Epoch 8/150\n",
      "21964/21964 [==============================] - 1s 57us/step - loss: 1.0003 - acc: 0.1083 - val_loss: 1.0003 - val_acc: 0.1142\n",
      "Epoch 9/150\n",
      "21964/21964 [==============================] - 1s 54us/step - loss: 1.0003 - acc: 0.1212 - val_loss: 1.0003 - val_acc: 0.1258\n",
      "Epoch 10/150\n",
      "21964/21964 [==============================] - 1s 53us/step - loss: 1.0003 - acc: 0.1365 - val_loss: 1.0003 - val_acc: 0.1430\n",
      "Epoch 11/150\n",
      "21964/21964 [==============================] - 1s 53us/step - loss: 1.0003 - acc: 0.1509 - val_loss: 1.0002 - val_acc: 0.1543\n",
      "Epoch 12/150\n",
      "21964/21964 [==============================] - 1s 55us/step - loss: 1.0002 - acc: 0.1665 - val_loss: 1.0002 - val_acc: 0.1685\n",
      "Epoch 13/150\n",
      "21964/21964 [==============================] - 1s 58us/step - loss: 1.0002 - acc: 0.1779 - val_loss: 1.0002 - val_acc: 0.1823\n",
      "Epoch 14/150\n",
      "21964/21964 [==============================] - 1s 57us/step - loss: 1.0002 - acc: 0.1877 - val_loss: 1.0002 - val_acc: 0.1916\n",
      "Epoch 15/150\n",
      "21964/21964 [==============================] - 1s 52us/step - loss: 1.0002 - acc: 0.1993 - val_loss: 1.0002 - val_acc: 0.2027\n",
      "Epoch 16/150\n",
      "21964/21964 [==============================] - 1s 53us/step - loss: 1.0002 - acc: 0.2114 - val_loss: 1.0002 - val_acc: 0.2158\n",
      "Epoch 17/150\n",
      "21964/21964 [==============================] - 1s 54us/step - loss: 1.0002 - acc: 0.2233 - val_loss: 1.0002 - val_acc: 0.2244\n",
      "Epoch 18/150\n",
      "21964/21964 [==============================] - 1s 52us/step - loss: 1.0001 - acc: 0.2312 - val_loss: 1.0001 - val_acc: 0.2360\n",
      "Epoch 19/150\n",
      "21964/21964 [==============================] - 1s 56us/step - loss: 1.0001 - acc: 0.2410 - val_loss: 1.0001 - val_acc: 0.2431\n",
      "Epoch 20/150\n",
      "21964/21964 [==============================] - 1s 53us/step - loss: 1.0001 - acc: 0.2496 - val_loss: 1.0001 - val_acc: 0.2502\n",
      "Epoch 21/150\n",
      "21964/21964 [==============================] - 1s 55us/step - loss: 1.0001 - acc: 0.2591 - val_loss: 1.0001 - val_acc: 0.2568\n",
      "Epoch 22/150\n",
      "21964/21964 [==============================] - 1s 54us/step - loss: 1.0001 - acc: 0.2683 - val_loss: 1.0001 - val_acc: 0.2628\n",
      "Epoch 23/150\n",
      "21964/21964 [==============================] - 1s 53us/step - loss: 1.0001 - acc: 0.2764 - val_loss: 1.0001 - val_acc: 0.2741\n",
      "Epoch 24/150\n",
      "21964/21964 [==============================] - 1s 54us/step - loss: 1.0001 - acc: 0.2870 - val_loss: 1.0001 - val_acc: 0.2830\n",
      "Epoch 25/150\n",
      "21964/21964 [==============================] - 1s 54us/step - loss: 1.0001 - acc: 0.2966 - val_loss: 1.0001 - val_acc: 0.2914\n",
      "Epoch 26/150\n",
      "21964/21964 [==============================] - 1s 54us/step - loss: 1.0001 - acc: 0.3050 - val_loss: 1.0001 - val_acc: 0.3005\n",
      "Epoch 27/150\n",
      "21964/21964 [==============================] - 1s 53us/step - loss: 1.0001 - acc: 0.3158 - val_loss: 1.0001 - val_acc: 0.3080\n",
      "Epoch 28/150\n",
      "21964/21964 [==============================] - 1s 54us/step - loss: 1.0001 - acc: 0.3230 - val_loss: 1.0001 - val_acc: 0.3169\n",
      "Epoch 29/150\n",
      "21964/21964 [==============================] - 1s 52us/step - loss: 1.0001 - acc: 0.3316 - val_loss: 1.0001 - val_acc: 0.3247\n",
      "Epoch 30/150\n",
      "21964/21964 [==============================] - 1s 55us/step - loss: 1.0001 - acc: 0.3382 - val_loss: 1.0001 - val_acc: 0.3302\n",
      "Epoch 31/150\n",
      "21964/21964 [==============================] - 1s 57us/step - loss: 1.0000 - acc: 0.3463 - val_loss: 1.0000 - val_acc: 0.3398\n",
      "Epoch 32/150\n",
      "21964/21964 [==============================] - 1s 55us/step - loss: 1.0000 - acc: 0.3527 - val_loss: 1.0000 - val_acc: 0.3475\n",
      "Epoch 33/150\n",
      "21964/21964 [==============================] - 1s 53us/step - loss: 1.0000 - acc: 0.3591 - val_loss: 1.0000 - val_acc: 0.3526\n",
      "Epoch 34/150\n",
      "21964/21964 [==============================] - 1s 56us/step - loss: 1.0000 - acc: 0.3648 - val_loss: 1.0000 - val_acc: 0.3584\n",
      "Epoch 35/150\n",
      "21964/21964 [==============================] - 1s 54us/step - loss: 1.0000 - acc: 0.3709 - val_loss: 1.0000 - val_acc: 0.3684\n",
      "Epoch 36/150\n",
      "21964/21964 [==============================] - 1s 53us/step - loss: 1.0000 - acc: 0.3762 - val_loss: 1.0000 - val_acc: 0.3739\n",
      "Epoch 37/150\n",
      "21964/21964 [==============================] - 1s 53us/step - loss: 1.0000 - acc: 0.3806 - val_loss: 1.0000 - val_acc: 0.3779\n",
      "Epoch 38/150\n",
      "21964/21964 [==============================] - 1s 54us/step - loss: 1.0000 - acc: 0.3869 - val_loss: 1.0000 - val_acc: 0.3808\n",
      "Epoch 39/150\n",
      "21964/21964 [==============================] - 1s 55us/step - loss: 1.0000 - acc: 0.3903 - val_loss: 1.0000 - val_acc: 0.3872\n",
      "Epoch 40/150\n",
      "21964/21964 [==============================] - 1s 53us/step - loss: 1.0000 - acc: 0.3962 - val_loss: 1.0000 - val_acc: 0.3875\n",
      "Epoch 41/150\n",
      "21964/21964 [==============================] - ETA: 0s - loss: 1.0000 - acc: 0.400 - 1s 55us/step - loss: 1.0000 - acc: 0.4003 - val_loss: 1.0000 - val_acc: 0.3950\n",
      "Epoch 42/150\n",
      "21964/21964 [==============================] - 1s 52us/step - loss: 1.0000 - acc: 0.4053 - val_loss: 1.0000 - val_acc: 0.3968\n",
      "Epoch 43/150\n",
      "21964/21964 [==============================] - 1s 55us/step - loss: 1.0000 - acc: 0.4104 - val_loss: 1.0000 - val_acc: 0.4047\n",
      "Epoch 44/150\n",
      "21964/21964 [==============================] - 1s 54us/step - loss: 1.0000 - acc: 0.4136 - val_loss: 1.0000 - val_acc: 0.4094\n",
      "Epoch 45/150\n",
      "21964/21964 [==============================] - 1s 54us/step - loss: 1.0000 - acc: 0.4189 - val_loss: 1.0000 - val_acc: 0.4116\n",
      "Epoch 46/150\n",
      "21964/21964 [==============================] - 1s 53us/step - loss: 1.0000 - acc: 0.4217 - val_loss: 1.0000 - val_acc: 0.4169\n",
      "Epoch 47/150\n",
      "21964/21964 [==============================] - 1s 55us/step - loss: 1.0000 - acc: 0.4260 - val_loss: 1.0000 - val_acc: 0.4209\n",
      "Epoch 48/150\n",
      "21964/21964 [==============================] - 1s 59us/step - loss: 1.0000 - acc: 0.4309 - val_loss: 1.0000 - val_acc: 0.4203\n",
      "Epoch 49/150\n",
      "21964/21964 [==============================] - 1s 55us/step - loss: 0.9999 - acc: 0.4325 - val_loss: 1.0000 - val_acc: 0.4251\n",
      "Epoch 50/150\n",
      "21964/21964 [==============================] - 1s 54us/step - loss: 0.9999 - acc: 0.4364 - val_loss: 0.9999 - val_acc: 0.4262\n",
      "Epoch 51/150\n",
      "21964/21964 [==============================] - 1s 53us/step - loss: 0.9999 - acc: 0.4389 - val_loss: 0.9999 - val_acc: 0.4354\n",
      "Epoch 52/150\n",
      "21964/21964 [==============================] - 1s 55us/step - loss: 0.9999 - acc: 0.4403 - val_loss: 0.9999 - val_acc: 0.4416\n",
      "Epoch 53/150\n",
      "21964/21964 [==============================] - 1s 54us/step - loss: 0.9999 - acc: 0.4458 - val_loss: 0.9999 - val_acc: 0.4351\n",
      "Epoch 54/150\n",
      "21964/21964 [==============================] - 1s 56us/step - loss: 0.9999 - acc: 0.4454 - val_loss: 0.9999 - val_acc: 0.4365\n",
      "Epoch 55/150\n",
      "21964/21964 [==============================] - 1s 53us/step - loss: 0.9999 - acc: 0.4491 - val_loss: 0.9999 - val_acc: 0.4425\n",
      "Epoch 56/150\n",
      "21964/21964 [==============================] - 1s 57us/step - loss: 0.9999 - acc: 0.4502 - val_loss: 0.9999 - val_acc: 0.4506\n",
      "Epoch 57/150\n",
      "21964/21964 [==============================] - 1s 54us/step - loss: 0.9999 - acc: 0.4558 - val_loss: 0.9999 - val_acc: 0.4460\n",
      "Epoch 58/150\n",
      "21964/21964 [==============================] - 1s 55us/step - loss: 0.9999 - acc: 0.4557 - val_loss: 0.9999 - val_acc: 0.4496\n",
      "Epoch 59/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21964/21964 [==============================] - 1s 53us/step - loss: 0.9999 - acc: 0.4574 - val_loss: 0.9999 - val_acc: 0.4520\n",
      "Epoch 60/150\n",
      "21964/21964 [==============================] - 1s 54us/step - loss: 0.9999 - acc: 0.4606 - val_loss: 0.9999 - val_acc: 0.4522\n",
      "Epoch 61/150\n",
      "21964/21964 [==============================] - 1s 54us/step - loss: 0.9999 - acc: 0.4633 - val_loss: 0.9999 - val_acc: 0.4578\n",
      "Epoch 62/150\n",
      "21964/21964 [==============================] - 1s 56us/step - loss: 0.9998 - acc: 0.4665 - val_loss: 0.9999 - val_acc: 0.4637\n",
      "Epoch 63/150\n",
      "21964/21964 [==============================] - 1s 58us/step - loss: 0.9998 - acc: 0.4674 - val_loss: 0.9998 - val_acc: 0.4598\n",
      "Epoch 64/150\n",
      "21964/21964 [==============================] - 1s 56us/step - loss: 0.9998 - acc: 0.4674 - val_loss: 0.9998 - val_acc: 0.4671\n",
      "Epoch 65/150\n",
      "21964/21964 [==============================] - 1s 62us/step - loss: 0.9998 - acc: 0.4695 - val_loss: 0.9998 - val_acc: 0.4655\n",
      "Epoch 66/150\n",
      "21964/21964 [==============================] - 1s 57us/step - loss: 0.9998 - acc: 0.4711 - val_loss: 0.9998 - val_acc: 0.4629\n",
      "Epoch 67/150\n",
      "21964/21964 [==============================] - 1s 57us/step - loss: 0.9998 - acc: 0.4728 - val_loss: 0.9998 - val_acc: 0.4691\n",
      "Epoch 68/150\n",
      "21964/21964 [==============================] - 1s 59us/step - loss: 0.9998 - acc: 0.4726 - val_loss: 0.9998 - val_acc: 0.4686\n",
      "Epoch 69/150\n",
      "21964/21964 [==============================] - 1s 56us/step - loss: 0.9997 - acc: 0.4749 - val_loss: 0.9998 - val_acc: 0.4706\n",
      "Epoch 70/150\n",
      "21964/21964 [==============================] - 1s 52us/step - loss: 0.9997 - acc: 0.4771 - val_loss: 0.9997 - val_acc: 0.4699\n",
      "Epoch 71/150\n",
      "21964/21964 [==============================] - 1s 50us/step - loss: 0.9997 - acc: 0.4771 - val_loss: 0.9997 - val_acc: 0.4711\n",
      "Epoch 72/150\n",
      "21964/21964 [==============================] - 1s 49us/step - loss: 0.9997 - acc: 0.4779 - val_loss: 0.9997 - val_acc: 0.4730\n",
      "Epoch 73/150\n",
      "21964/21964 [==============================] - 1s 47us/step - loss: 0.9996 - acc: 0.4793 - val_loss: 0.9997 - val_acc: 0.4740\n",
      "Epoch 74/150\n",
      "21964/21964 [==============================] - 1s 48us/step - loss: 0.9996 - acc: 0.4771 - val_loss: 0.9996 - val_acc: 0.4740\n",
      "Epoch 75/150\n",
      "21964/21964 [==============================] - 1s 48us/step - loss: 0.9996 - acc: 0.4773 - val_loss: 0.9996 - val_acc: 0.4757\n",
      "Epoch 76/150\n",
      "21964/21964 [==============================] - 1s 48us/step - loss: 0.9995 - acc: 0.4781 - val_loss: 0.9996 - val_acc: 0.4731\n",
      "Epoch 77/150\n",
      "21964/21964 [==============================] - 1s 48us/step - loss: 0.9995 - acc: 0.4766 - val_loss: 0.9995 - val_acc: 0.4726\n",
      "Epoch 78/150\n",
      "21964/21964 [==============================] - 1s 47us/step - loss: 0.9994 - acc: 0.4764 - val_loss: 0.9994 - val_acc: 0.4657\n",
      "Epoch 79/150\n",
      "21964/21964 [==============================] - 1s 49us/step - loss: 0.9993 - acc: 0.4741 - val_loss: 0.9994 - val_acc: 0.4608\n",
      "Epoch 80/150\n",
      "21964/21964 [==============================] - 1s 47us/step - loss: 0.9992 - acc: 0.4708 - val_loss: 0.9992 - val_acc: 0.4629\n",
      "Epoch 81/150\n",
      "21964/21964 [==============================] - 1s 48us/step - loss: 0.9991 - acc: 0.4671 - val_loss: 0.9991 - val_acc: 0.4542\n",
      "Epoch 82/150\n",
      "21964/21964 [==============================] - 1s 50us/step - loss: 0.9989 - acc: 0.4589 - val_loss: 0.9989 - val_acc: 0.4467\n",
      "Epoch 83/150\n",
      "21964/21964 [==============================] - 1s 48us/step - loss: 0.9985 - acc: 0.4494 - val_loss: 0.9984 - val_acc: 0.4342\n",
      "Epoch 84/150\n",
      "21964/21964 [==============================] - 1s 52us/step - loss: 0.9976 - acc: 0.4306 - val_loss: 0.9970 - val_acc: 0.4043\n",
      "Epoch 85/150\n",
      "21964/21964 [==============================] - 1s 47us/step - loss: 0.9930 - acc: 0.3886 - val_loss: 0.9888 - val_acc: 0.3416\n",
      "Epoch 86/150\n",
      "21964/21964 [==============================] - 1s 49us/step - loss: 0.9828 - acc: 0.3727 - val_loss: 0.9815 - val_acc: 0.3733\n",
      "Epoch 87/150\n",
      "21964/21964 [==============================] - 1s 53us/step - loss: 0.9769 - acc: 0.3922 - val_loss: 0.9775 - val_acc: 0.3901\n",
      "Epoch 88/150\n",
      "21964/21964 [==============================] - 1s 51us/step - loss: 0.9732 - acc: 0.4052 - val_loss: 0.9746 - val_acc: 0.3923\n",
      "Epoch 89/150\n",
      "21964/21964 [==============================] - 1s 53us/step - loss: 0.9703 - acc: 0.4084 - val_loss: 0.9721 - val_acc: 0.3997\n",
      "Epoch 90/150\n",
      "21964/21964 [==============================] - 1s 62us/step - loss: 0.9679 - acc: 0.4162 - val_loss: 0.9697 - val_acc: 0.4114\n",
      "Epoch 91/150\n",
      "21964/21964 [==============================] - 1s 61us/step - loss: 0.9658 - acc: 0.4232 - val_loss: 0.9678 - val_acc: 0.4187\n",
      "Epoch 92/150\n",
      "21964/21964 [==============================] - 1s 59us/step - loss: 0.9642 - acc: 0.4296 - val_loss: 0.9663 - val_acc: 0.4292\n",
      "Epoch 93/150\n",
      "21964/21964 [==============================] - 1s 56us/step - loss: 0.9629 - acc: 0.4349 - val_loss: 0.9650 - val_acc: 0.4298\n",
      "Epoch 94/150\n",
      "21964/21964 [==============================] - 1s 56us/step - loss: 0.9619 - acc: 0.4370 - val_loss: 0.9641 - val_acc: 0.4440\n",
      "Epoch 95/150\n",
      "21964/21964 [==============================] - 1s 54us/step - loss: 0.9611 - acc: 0.4438 - val_loss: 0.9633 - val_acc: 0.4442\n",
      "Epoch 96/150\n",
      "21964/21964 [==============================] - 1s 57us/step - loss: 0.9604 - acc: 0.4499 - val_loss: 0.9627 - val_acc: 0.4516\n",
      "Epoch 97/150\n",
      "21964/21964 [==============================] - 1s 57us/step - loss: 0.9600 - acc: 0.4556 - val_loss: 0.9622 - val_acc: 0.4609\n",
      "Epoch 98/150\n",
      "21964/21964 [==============================] - 1s 58us/step - loss: 0.9596 - acc: 0.4613 - val_loss: 0.9619 - val_acc: 0.4608\n",
      "Epoch 99/150\n",
      "21964/21964 [==============================] - 1s 58us/step - loss: 0.9594 - acc: 0.4643 - val_loss: 0.9616 - val_acc: 0.4649\n",
      "Epoch 100/150\n",
      "21964/21964 [==============================] - 1s 61us/step - loss: 0.9591 - acc: 0.4715 - val_loss: 0.9615 - val_acc: 0.4675\n",
      "Epoch 101/150\n",
      "21964/21964 [==============================] - 1s 56us/step - loss: 0.9590 - acc: 0.4772 - val_loss: 0.9613 - val_acc: 0.4600\n",
      "Epoch 102/150\n",
      "21964/21964 [==============================] - 1s 56us/step - loss: 0.9589 - acc: 0.4788 - val_loss: 0.9612 - val_acc: 0.4702\n",
      "Epoch 103/150\n",
      "21964/21964 [==============================] - 1s 56us/step - loss: 0.9588 - acc: 0.4867 - val_loss: 0.9611 - val_acc: 0.4677\n",
      "Epoch 104/150\n",
      "21964/21964 [==============================] - 1s 58us/step - loss: 0.9587 - acc: 0.4879 - val_loss: 0.9610 - val_acc: 0.4757\n",
      "Epoch 105/150\n",
      "21964/21964 [==============================] - 1s 57us/step - loss: 0.9586 - acc: 0.4913 - val_loss: 0.9610 - val_acc: 0.4708\n",
      "Epoch 106/150\n",
      "21964/21964 [==============================] - 1s 55us/step - loss: 0.9586 - acc: 0.4920 - val_loss: 0.9609 - val_acc: 0.4817\n",
      "Epoch 107/150\n",
      "21964/21964 [==============================] - 1s 55us/step - loss: 0.9585 - acc: 0.4958 - val_loss: 0.9608 - val_acc: 0.4781\n",
      "Epoch 108/150\n",
      "21964/21964 [==============================] - 1s 55us/step - loss: 0.9585 - acc: 0.4978 - val_loss: 0.9608 - val_acc: 0.4766\n",
      "Epoch 109/150\n",
      "21964/21964 [==============================] - 1s 57us/step - loss: 0.9584 - acc: 0.4996 - val_loss: 0.9607 - val_acc: 0.4762\n",
      "Epoch 110/150\n",
      "21964/21964 [==============================] - 1s 57us/step - loss: 0.9584 - acc: 0.4990 - val_loss: 0.9606 - val_acc: 0.4883\n",
      "Epoch 111/150\n",
      "21964/21964 [==============================] - 1s 56us/step - loss: 0.9583 - acc: 0.5006 - val_loss: 0.9605 - val_acc: 0.4791\n",
      "Epoch 112/150\n",
      "21964/21964 [==============================] - 1s 54us/step - loss: 0.9581 - acc: 0.4969 - val_loss: 0.9603 - val_acc: 0.4915\n",
      "Epoch 113/150\n",
      "21964/21964 [==============================] - 1s 55us/step - loss: 0.9579 - acc: 0.4991 - val_loss: 0.9601 - val_acc: 0.4832\n",
      "Epoch 114/150\n",
      "21964/21964 [==============================] - 1s 55us/step - loss: 0.9578 - acc: 0.4999 - val_loss: 0.9599 - val_acc: 0.4817\n",
      "Epoch 115/150\n",
      "21964/21964 [==============================] - 1s 55us/step - loss: 0.9576 - acc: 0.4976 - val_loss: 0.9598 - val_acc: 0.4833\n",
      "Epoch 116/150\n",
      "21964/21964 [==============================] - 1s 55us/step - loss: 0.9575 - acc: 0.4965 - val_loss: 0.9597 - val_acc: 0.4842\n",
      "Epoch 117/150\n",
      "21964/21964 [==============================] - 1s 56us/step - loss: 0.9575 - acc: 0.4979 - val_loss: 0.9596 - val_acc: 0.4852\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 118/150\n",
      "21964/21964 [==============================] - 1s 59us/step - loss: 0.9574 - acc: 0.4973 - val_loss: 0.9595 - val_acc: 0.4826\n",
      "Epoch 119/150\n",
      "21964/21964 [==============================] - 1s 59us/step - loss: 0.9573 - acc: 0.4943 - val_loss: 0.9594 - val_acc: 0.4872\n",
      "Epoch 120/150\n",
      "21964/21964 [==============================] - 1s 58us/step - loss: 0.9572 - acc: 0.4946 - val_loss: 0.9593 - val_acc: 0.4830\n",
      "Epoch 121/150\n",
      "21964/21964 [==============================] - 1s 58us/step - loss: 0.9571 - acc: 0.4939 - val_loss: 0.9592 - val_acc: 0.4817\n",
      "Epoch 122/150\n",
      "21964/21964 [==============================] - 1s 60us/step - loss: 0.9569 - acc: 0.4935 - val_loss: 0.9590 - val_acc: 0.4764\n",
      "Epoch 123/150\n",
      "21964/21964 [==============================] - 1s 58us/step - loss: 0.9567 - acc: 0.4921 - val_loss: 0.9587 - val_acc: 0.4762\n",
      "Epoch 124/150\n",
      "21964/21964 [==============================] - 1s 57us/step - loss: 0.9563 - acc: 0.4871 - val_loss: 0.9583 - val_acc: 0.4679\n",
      "Epoch 125/150\n",
      "21964/21964 [==============================] - 1s 55us/step - loss: 0.9554 - acc: 0.4774 - val_loss: 0.9571 - val_acc: 0.4580\n",
      "Epoch 126/150\n",
      "21964/21964 [==============================] - 1s 57us/step - loss: 0.9519 - acc: 0.4529 - val_loss: 0.9511 - val_acc: 0.4079\n",
      "Epoch 127/150\n",
      "21964/21964 [==============================] - 1s 53us/step - loss: 0.9424 - acc: 0.4322 - val_loss: 0.9431 - val_acc: 0.4313\n",
      "Epoch 128/150\n",
      "21964/21964 [==============================] - 1s 54us/step - loss: 0.9354 - acc: 0.4521 - val_loss: 0.9380 - val_acc: 0.4487\n",
      "Epoch 129/150\n",
      "21964/21964 [==============================] - 1s 55us/step - loss: 0.9308 - acc: 0.4650 - val_loss: 0.9343 - val_acc: 0.4560\n",
      "Epoch 130/150\n",
      "21964/21964 [==============================] - 1s 54us/step - loss: 0.9275 - acc: 0.4721 - val_loss: 0.9315 - val_acc: 0.4617\n",
      "Epoch 131/150\n",
      "21964/21964 [==============================] - 1s 55us/step - loss: 0.9250 - acc: 0.4733 - val_loss: 0.9294 - val_acc: 0.4566\n",
      "Epoch 132/150\n",
      "21964/21964 [==============================] - 1s 55us/step - loss: 0.9230 - acc: 0.4742 - val_loss: 0.9276 - val_acc: 0.4626\n",
      "Epoch 133/150\n",
      "21964/21964 [==============================] - 1s 56us/step - loss: 0.9212 - acc: 0.4766 - val_loss: 0.9261 - val_acc: 0.4604\n",
      "Epoch 134/150\n",
      "21964/21964 [==============================] - 1s 56us/step - loss: 0.9195 - acc: 0.4768 - val_loss: 0.9246 - val_acc: 0.4644\n",
      "Epoch 135/150\n",
      "21964/21964 [==============================] - 1s 53us/step - loss: 0.9176 - acc: 0.4760 - val_loss: 0.9229 - val_acc: 0.4564\n",
      "Epoch 136/150\n",
      "21964/21964 [==============================] - 1s 53us/step - loss: 0.9149 - acc: 0.4700 - val_loss: 0.9200 - val_acc: 0.4542\n",
      "Epoch 137/150\n",
      "21964/21964 [==============================] - 1s 53us/step - loss: 0.9098 - acc: 0.4572 - val_loss: 0.9107 - val_acc: 0.4407\n",
      "Epoch 138/150\n",
      "21964/21964 [==============================] - 1s 53us/step - loss: 0.8991 - acc: 0.4581 - val_loss: 0.9008 - val_acc: 0.4511\n",
      "Epoch 139/150\n",
      "21964/21964 [==============================] - 1s 55us/step - loss: 0.8920 - acc: 0.4755 - val_loss: 0.8952 - val_acc: 0.4675\n",
      "Epoch 140/150\n",
      "21964/21964 [==============================] - 1s 54us/step - loss: 0.8880 - acc: 0.4886 - val_loss: 0.8915 - val_acc: 0.4748\n",
      "Epoch 141/150\n",
      "21964/21964 [==============================] - 1s 54us/step - loss: 0.8850 - acc: 0.4952 - val_loss: 0.8888 - val_acc: 0.4795\n",
      "Epoch 142/150\n",
      "21964/21964 [==============================] - 1s 52us/step - loss: 0.8828 - acc: 0.4990 - val_loss: 0.8867 - val_acc: 0.4852\n",
      "Epoch 143/150\n",
      "21964/21964 [==============================] - 1s 55us/step - loss: 0.8809 - acc: 0.4983 - val_loss: 0.8850 - val_acc: 0.4824\n",
      "Epoch 144/150\n",
      "21964/21964 [==============================] - 1s 53us/step - loss: 0.8794 - acc: 0.4991 - val_loss: 0.8835 - val_acc: 0.4892\n",
      "Epoch 145/150\n",
      "21964/21964 [==============================] - 1s 54us/step - loss: 0.8780 - acc: 0.4986 - val_loss: 0.8820 - val_acc: 0.4795\n",
      "Epoch 146/150\n",
      "21964/21964 [==============================] - 1s 53us/step - loss: 0.8765 - acc: 0.4943 - val_loss: 0.8804 - val_acc: 0.4740\n",
      "Epoch 147/150\n",
      "21964/21964 [==============================] - 1s 55us/step - loss: 0.8745 - acc: 0.4902 - val_loss: 0.8775 - val_acc: 0.4662\n",
      "Epoch 148/150\n",
      "21964/21964 [==============================] - 1s 62us/step - loss: 0.8703 - acc: 0.4738 - val_loss: 0.8713 - val_acc: 0.4629\n",
      "Epoch 149/150\n",
      "21964/21964 [==============================] - 1s 56us/step - loss: 0.8629 - acc: 0.4771 - val_loss: 0.8627 - val_acc: 0.4586\n",
      "Epoch 150/150\n",
      "21964/21964 [==============================] - 1s 55us/step - loss: 0.8527 - acc: 0.4706 - val_loss: 0.8512 - val_acc: 0.4722\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2af86d3e780>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils.np_utils import to_categorical\n",
    "#Con la data pre-procesada capa inicial con activacion lineal con loss categorical_hinge no supero 0,49 \n",
    "model7 = Sequential()\n",
    "model7.add(Dense(30, input_dim=Sx_tr.shape[1], init='uniform', activation='relu'))\n",
    "model7.add(Dense(30, init='uniform', activation='elu'))\n",
    "model7.add(Dense(25, init='uniform', activation='softmax'))\n",
    "model7.compile(optimizer=SGD(lr=0.05), loss='categorical_hinge', metrics=['accuracy'])\n",
    "model7.fit(Sx_tr, to_categorical(y_tr), nb_epoch=150, batch_size=128, verbose=1,\n",
    "                    validation_data=(Sx_v,to_categorical(y_v)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alfredo\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:13: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(30, input_dim=784, activation=\"relu\", kernel_initializer=\"uniform\")`\n",
      "  del sys.path[0]\n",
      "C:\\Users\\Alfredo\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:14: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(30, activation=\"elu\", kernel_initializer=\"uniform\")`\n",
      "  \n",
      "C:\\Users\\Alfredo\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:15: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(25, activation=\"softmax\", kernel_initializer=\"uniform\")`\n",
      "  from ipykernel import kernelapp as app\n",
      "C:\\Users\\Alfredo\\Anaconda3\\lib\\site-packages\\keras\\models.py:939: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 21964 samples, validate on 5491 samples\n",
      "Epoch 1/100\n",
      "21964/21964 [==============================] - 2s 96us/step - loss: 1.9859 - acc: 0.3908 - val_loss: 1.2989 - val_acc: 0.5797\n",
      "Epoch 2/100\n",
      "21964/21964 [==============================] - 1s 56us/step - loss: 0.9384 - acc: 0.6910 - val_loss: 0.7088 - val_acc: 0.7687\n",
      "Epoch 3/100\n",
      "21964/21964 [==============================] - 1s 53us/step - loss: 0.5152 - acc: 0.8486 - val_loss: 0.4111 - val_acc: 0.8818\n",
      "Epoch 4/100\n",
      "21964/21964 [==============================] - 1s 54us/step - loss: 0.2720 - acc: 0.9315 - val_loss: 0.2007 - val_acc: 0.9548\n",
      "Epoch 5/100\n",
      "21964/21964 [==============================] - 1s 55us/step - loss: 0.1266 - acc: 0.9758 - val_loss: 0.0905 - val_acc: 0.9878\n",
      "Epoch 6/100\n",
      "21964/21964 [==============================] - 1s 61us/step - loss: 0.0489 - acc: 0.9955 - val_loss: 0.0313 - val_acc: 0.9989\n",
      "Epoch 7/100\n",
      "21964/21964 [==============================] - 1s 57us/step - loss: 0.0179 - acc: 0.9991 - val_loss: 0.0163 - val_acc: 0.9985\n",
      "Epoch 8/100\n",
      "21964/21964 [==============================] - 1s 54us/step - loss: 0.0090 - acc: 0.9993 - val_loss: 0.0058 - val_acc: 0.9998\n",
      "Epoch 9/100\n",
      "21964/21964 [==============================] - 1s 55us/step - loss: 0.0052 - acc: 0.9994 - val_loss: 0.0042 - val_acc: 0.9998\n",
      "Epoch 10/100\n",
      "21964/21964 [==============================] - 1s 54us/step - loss: 0.0029 - acc: 0.9997 - val_loss: 0.0033 - val_acc: 0.9996\n",
      "Epoch 11/100\n",
      "21964/21964 [==============================] - 1s 56us/step - loss: 0.0015 - acc: 0.9999 - val_loss: 0.0013 - val_acc: 1.0000\n",
      "Epoch 12/100\n",
      "21964/21964 [==============================] - 2s 70us/step - loss: 0.0026 - acc: 0.9993 - val_loss: 0.0014 - val_acc: 0.9998\n",
      "Epoch 13/100\n",
      "21964/21964 [==============================] - 1s 63us/step - loss: 6.6488e-04 - acc: 0.9999 - val_loss: 5.1196e-04 - val_acc: 1.0000\n",
      "Epoch 14/100\n",
      "21964/21964 [==============================] - 1s 63us/step - loss: 0.0012 - acc: 0.9997 - val_loss: 3.7986e-04 - val_acc: 1.0000\n",
      "Epoch 15/100\n",
      "21964/21964 [==============================] - 1s 63us/step - loss: 0.0021 - acc: 0.9994 - val_loss: 3.7960e-04 - val_acc: 1.0000\n",
      "Epoch 16/100\n",
      "21964/21964 [==============================] - 1s 63us/step - loss: 6.6360e-04 - acc: 0.9997 - val_loss: 1.6256e-04 - val_acc: 1.0000\n",
      "Epoch 17/100\n",
      "21964/21964 [==============================] - 1s 56us/step - loss: 0.0010 - acc: 0.9997 - val_loss: 2.9871e-04 - val_acc: 1.0000\n",
      "Epoch 18/100\n",
      "21964/21964 [==============================] - 1s 51us/step - loss: 6.0841e-04 - acc: 0.9998 - val_loss: 1.5041e-04 - val_acc: 1.0000\n",
      "Epoch 19/100\n",
      "21964/21964 [==============================] - 1s 51us/step - loss: 3.7725e-04 - acc: 0.9999 - val_loss: 8.0662e-05 - val_acc: 1.0000\n",
      "Epoch 20/100\n",
      "21964/21964 [==============================] - 1s 52us/step - loss: 9.6566e-04 - acc: 0.9997 - val_loss: 0.0011 - val_acc: 0.9996\n",
      "Epoch 21/100\n",
      "21964/21964 [==============================] - 1s 51us/step - loss: 4.5000e-04 - acc: 0.9998 - val_loss: 0.0215 - val_acc: 0.9954\n",
      "Epoch 22/100\n",
      "21964/21964 [==============================] - 1s 51us/step - loss: 0.0012 - acc: 0.9996 - val_loss: 0.0020 - val_acc: 0.9991\n",
      "Epoch 23/100\n",
      "21964/21964 [==============================] - 1s 51us/step - loss: 8.2142e-04 - acc: 0.9997 - val_loss: 9.3538e-05 - val_acc: 1.0000\n",
      "Epoch 24/100\n",
      "21964/21964 [==============================] - 1s 51us/step - loss: 8.2938e-04 - acc: 0.9999 - val_loss: 1.6538e-04 - val_acc: 1.0000\n",
      "Epoch 25/100\n",
      "21964/21964 [==============================] - 1s 51us/step - loss: 0.0011 - acc: 0.9996 - val_loss: 9.9426e-04 - val_acc: 0.9996\n",
      "Epoch 26/100\n",
      "21964/21964 [==============================] - 1s 51us/step - loss: 2.5874e-04 - acc: 0.9999 - val_loss: 2.3798e-04 - val_acc: 1.0000\n",
      "Epoch 27/100\n",
      "21964/21964 [==============================] - 1s 52us/step - loss: 0.0016 - acc: 0.9997 - val_loss: 0.0021 - val_acc: 0.9993\n",
      "Epoch 28/100\n",
      "21964/21964 [==============================] - 1s 55us/step - loss: 2.0397e-04 - acc: 0.9999 - val_loss: 8.9522e-04 - val_acc: 0.9995\n",
      "Epoch 29/100\n",
      "21964/21964 [==============================] - 1s 63us/step - loss: 6.7874e-04 - acc: 0.9997 - val_loss: 3.8475e-05 - val_acc: 1.0000\n",
      "Epoch 30/100\n",
      "21964/21964 [==============================] - 1s 59us/step - loss: 2.4430e-04 - acc: 0.9999 - val_loss: 1.5511e-04 - val_acc: 0.9998\n",
      "Epoch 31/100\n",
      "21964/21964 [==============================] - 1s 57us/step - loss: 0.0016 - acc: 0.9995 - val_loss: 4.8694e-05 - val_acc: 1.0000\n",
      "Epoch 32/100\n",
      "21964/21964 [==============================] - 1s 56us/step - loss: 8.3591e-04 - acc: 0.9997 - val_loss: 1.1696e-04 - val_acc: 1.0000\n",
      "Epoch 33/100\n",
      "21964/21964 [==============================] - 1s 58us/step - loss: 0.0011 - acc: 0.9997 - val_loss: 5.7876e-05 - val_acc: 1.0000\n",
      "Epoch 34/100\n",
      "21964/21964 [==============================] - 1s 59us/step - loss: 3.1911e-04 - acc: 0.9999 - val_loss: 2.4961e-04 - val_acc: 0.9998\n",
      "Epoch 35/100\n",
      "21964/21964 [==============================] - 1s 56us/step - loss: 3.4504e-04 - acc: 0.9998 - val_loss: 0.0027 - val_acc: 0.9989\n",
      "Epoch 36/100\n",
      "21964/21964 [==============================] - 1s 55us/step - loss: 2.2314e-05 - acc: 1.0000 - val_loss: 1.1859e-05 - val_acc: 1.0000\n",
      "Epoch 37/100\n",
      "21964/21964 [==============================] - 1s 53us/step - loss: 4.1311e-04 - acc: 0.9999 - val_loss: 4.6292e-04 - val_acc: 0.9998\n",
      "Epoch 38/100\n",
      "21964/21964 [==============================] - 1s 54us/step - loss: 1.7137e-04 - acc: 0.9999 - val_loss: 1.3579e-05 - val_acc: 1.0000\n",
      "Epoch 39/100\n",
      "21964/21964 [==============================] - 1s 53us/step - loss: 4.1628e-04 - acc: 0.9999 - val_loss: 7.4393e-05 - val_acc: 1.0000\n",
      "Epoch 40/100\n",
      "21964/21964 [==============================] - 1s 53us/step - loss: 4.4106e-04 - acc: 0.9999 - val_loss: 8.3690e-05 - val_acc: 1.0000\n",
      "Epoch 41/100\n",
      "21964/21964 [==============================] - 1s 56us/step - loss: 4.7063e-05 - acc: 1.0000 - val_loss: 6.7943e-06 - val_acc: 1.0000\n",
      "Epoch 42/100\n",
      "21964/21964 [==============================] - 1s 60us/step - loss: 0.0014 - acc: 0.9998 - val_loss: 2.5072e-04 - val_acc: 0.9998\n",
      "Epoch 43/100\n",
      "21964/21964 [==============================] - 1s 55us/step - loss: 1.3102e-04 - acc: 1.0000 - val_loss: 1.5993e-04 - val_acc: 1.0000\n",
      "Epoch 44/100\n",
      "21964/21964 [==============================] - 1s 54us/step - loss: 1.2647e-04 - acc: 0.9999 - val_loss: 9.5493e-04 - val_acc: 0.9998\n",
      "Epoch 45/100\n",
      "21964/21964 [==============================] - 1s 55us/step - loss: 7.5726e-05 - acc: 1.0000 - val_loss: 3.3712e-05 - val_acc: 1.0000\n",
      "Epoch 46/100\n",
      "21964/21964 [==============================] - 1s 58us/step - loss: 2.6410e-06 - acc: 1.0000 - val_loss: 4.0685e-05 - val_acc: 1.0000\n",
      "Epoch 47/100\n",
      "21964/21964 [==============================] - 1s 58us/step - loss: 2.6969e-04 - acc: 0.9999 - val_loss: 3.6077e-05 - val_acc: 1.0000\n",
      "Epoch 48/100\n",
      "21964/21964 [==============================] - 1s 59us/step - loss: 1.7969e-04 - acc: 1.0000 - val_loss: 9.7794e-05 - val_acc: 1.0000\n",
      "Epoch 49/100\n",
      "21964/21964 [==============================] - 1s 57us/step - loss: 1.1139e-04 - acc: 1.0000 - val_loss: 6.3635e-04 - val_acc: 0.9998\n",
      "Epoch 50/100\n",
      "21964/21964 [==============================] - 1s 58us/step - loss: 2.8756e-04 - acc: 0.9999 - val_loss: 3.1403e-04 - val_acc: 0.9998\n",
      "Epoch 51/100\n",
      "21964/21964 [==============================] - 1s 57us/step - loss: 7.3342e-04 - acc: 0.9998 - val_loss: 1.3559e-04 - val_acc: 1.0000\n",
      "Epoch 52/100\n",
      "21964/21964 [==============================] - 1s 57us/step - loss: 4.4541e-04 - acc: 0.9999 - val_loss: 9.6285e-04 - val_acc: 0.9998\n",
      "Epoch 53/100\n",
      "21964/21964 [==============================] - 1s 57us/step - loss: 0.0013 - acc: 0.9997 - val_loss: 0.0013 - val_acc: 0.9996\n",
      "Epoch 54/100\n",
      "21964/21964 [==============================] - 1s 58us/step - loss: 6.8785e-06 - acc: 1.0000 - val_loss: 6.1993e-04 - val_acc: 0.9998\n",
      "Epoch 55/100\n",
      "21964/21964 [==============================] - 1s 58us/step - loss: 6.2526e-04 - acc: 0.9999 - val_loss: 3.7037e-04 - val_acc: 0.9998\n",
      "Epoch 56/100\n",
      "21964/21964 [==============================] - 1s 60us/step - loss: 7.5327e-04 - acc: 0.9997 - val_loss: 5.9925e-05 - val_acc: 1.0000\n",
      "Epoch 57/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21964/21964 [==============================] - 1s 62us/step - loss: 0.0012 - acc: 0.9998 - val_loss: 6.6304e-04 - val_acc: 0.9996\n",
      "Epoch 58/100\n",
      "21964/21964 [==============================] - 1s 58us/step - loss: 3.9509e-05 - acc: 1.0000 - val_loss: 5.8092e-04 - val_acc: 0.9996\n",
      "Epoch 59/100\n",
      "21964/21964 [==============================] - 1s 54us/step - loss: 3.6355e-04 - acc: 0.9998 - val_loss: 2.1582e-05 - val_acc: 1.0000\n",
      "Epoch 60/100\n",
      "21964/21964 [==============================] - 1s 58us/step - loss: 3.9543e-04 - acc: 0.9999 - val_loss: 8.7052e-06 - val_acc: 1.0000\n",
      "Epoch 61/100\n",
      "21964/21964 [==============================] - 1s 62us/step - loss: 1.9756e-06 - acc: 1.0000 - val_loss: 2.2876e-06 - val_acc: 1.0000\n",
      "Epoch 62/100\n",
      "21964/21964 [==============================] - 1s 61us/step - loss: 0.0024 - acc: 0.9995 - val_loss: 4.4660e-04 - val_acc: 0.9998\n",
      "Epoch 63/100\n",
      "21964/21964 [==============================] - 1s 61us/step - loss: 5.4115e-05 - acc: 1.0000 - val_loss: 1.2854e-04 - val_acc: 1.0000\n",
      "Epoch 64/100\n",
      "21964/21964 [==============================] - 1s 58us/step - loss: 2.0171e-04 - acc: 0.9999 - val_loss: 4.6524e-04 - val_acc: 0.9998\n",
      "Epoch 65/100\n",
      "21964/21964 [==============================] - 1s 68us/step - loss: 0.0015 - acc: 0.9998 - val_loss: 8.2549e-05 - val_acc: 1.0000\n",
      "Epoch 66/100\n",
      "21964/21964 [==============================] - 1s 61us/step - loss: 5.2463e-04 - acc: 0.9999 - val_loss: 5.4434e-05 - val_acc: 1.0000\n",
      "Epoch 67/100\n",
      "21964/21964 [==============================] - 1s 52us/step - loss: 0.0015 - acc: 0.9998 - val_loss: 6.4799e-05 - val_acc: 1.0000\n",
      "Epoch 68/100\n",
      "21964/21964 [==============================] - 1s 51us/step - loss: 2.6876e-06 - acc: 1.0000 - val_loss: 8.1699e-06 - val_acc: 1.0000\n",
      "Epoch 69/100\n",
      "21964/21964 [==============================] - 1s 57us/step - loss: 3.4088e-04 - acc: 0.9999 - val_loss: 0.0050 - val_acc: 0.9991\n",
      "Epoch 70/100\n",
      "21964/21964 [==============================] - 1s 61us/step - loss: 1.5805e-05 - acc: 1.0000 - val_loss: 9.2820e-05 - val_acc: 1.0000\n",
      "Epoch 71/100\n",
      "21964/21964 [==============================] - 1s 68us/step - loss: 0.0014 - acc: 0.9997 - val_loss: 5.7959e-04 - val_acc: 0.9998\n",
      "Epoch 72/100\n",
      "21964/21964 [==============================] - 1s 67us/step - loss: 1.5935e-04 - acc: 0.9999 - val_loss: 6.8812e-05 - val_acc: 1.0000\n",
      "Epoch 73/100\n",
      "21964/21964 [==============================] - 1s 63us/step - loss: 3.1554e-04 - acc: 0.9998 - val_loss: 1.6528e-05 - val_acc: 1.0000\n",
      "Epoch 74/100\n",
      "21964/21964 [==============================] - 1s 60us/step - loss: 0.0013 - acc: 0.9997 - val_loss: 5.7620e-04 - val_acc: 0.9998\n",
      "Epoch 75/100\n",
      "21964/21964 [==============================] - 1s 61us/step - loss: 2.6721e-04 - acc: 1.0000 - val_loss: 9.2596e-05 - val_acc: 1.0000\n",
      "Epoch 76/100\n",
      "21964/21964 [==============================] - 1s 61us/step - loss: 4.2901e-06 - acc: 1.0000 - val_loss: 0.0017 - val_acc: 0.9996\n",
      "Epoch 77/100\n",
      "21964/21964 [==============================] - 1s 63us/step - loss: 5.6495e-04 - acc: 0.9999 - val_loss: 1.7943e-04 - val_acc: 0.9998\n",
      "Epoch 78/100\n",
      "21964/21964 [==============================] - 1s 65us/step - loss: 1.2409e-04 - acc: 1.0000 - val_loss: 7.5386e-04 - val_acc: 0.9998\n",
      "Epoch 79/100\n",
      "21964/21964 [==============================] - 1s 65us/step - loss: 6.4084e-04 - acc: 0.9998 - val_loss: 3.5621e-06 - val_acc: 1.0000\n",
      "Epoch 80/100\n",
      "21964/21964 [==============================] - 1s 60us/step - loss: 7.3473e-05 - acc: 1.0000 - val_loss: 9.3648e-05 - val_acc: 1.0000\n",
      "Epoch 81/100\n",
      "21964/21964 [==============================] - 1s 57us/step - loss: 6.2406e-04 - acc: 0.9998 - val_loss: 1.1750e-06 - val_acc: 1.0000\n",
      "Epoch 82/100\n",
      "21964/21964 [==============================] - 1s 57us/step - loss: 5.9294e-04 - acc: 0.9999 - val_loss: 0.0066 - val_acc: 0.9995\n",
      "Epoch 83/100\n",
      "21964/21964 [==============================] - 1s 51us/step - loss: 1.6992e-05 - acc: 1.0000 - val_loss: 4.9677e-06 - val_acc: 1.0000\n",
      "Epoch 84/100\n",
      "21964/21964 [==============================] - 1s 52us/step - loss: 4.2548e-07 - acc: 1.0000 - val_loss: 9.9992e-06 - val_acc: 1.0000\n",
      "Epoch 85/100\n",
      "21964/21964 [==============================] - 1s 52us/step - loss: 2.6852e-04 - acc: 1.0000 - val_loss: 4.1780e-05 - val_acc: 1.0000\n",
      "Epoch 86/100\n",
      "21964/21964 [==============================] - 1s 52us/step - loss: 1.6778e-04 - acc: 0.9999 - val_loss: 5.9653e-07 - val_acc: 1.0000\n",
      "Epoch 87/100\n",
      "21964/21964 [==============================] - 1s 52us/step - loss: 5.5430e-05 - acc: 1.0000 - val_loss: 8.1296e-07 - val_acc: 1.0000\n",
      "Epoch 88/100\n",
      "21964/21964 [==============================] - 1s 52us/step - loss: 5.4173e-04 - acc: 1.0000 - val_loss: 7.4382e-05 - val_acc: 1.0000\n",
      "Epoch 89/100\n",
      "21964/21964 [==============================] - 1s 52us/step - loss: 1.4315e-06 - acc: 1.0000 - val_loss: 2.2500e-06 - val_acc: 1.0000\n",
      "Epoch 90/100\n",
      "21964/21964 [==============================] - 1s 53us/step - loss: 0.0017 - acc: 0.9998 - val_loss: 5.5818e-06 - val_acc: 1.0000\n",
      "Epoch 91/100\n",
      "21964/21964 [==============================] - 1s 55us/step - loss: 2.4783e-04 - acc: 1.0000 - val_loss: 0.0123 - val_acc: 0.9975\n",
      "Epoch 92/100\n",
      "21964/21964 [==============================] - 1s 53us/step - loss: 2.1425e-04 - acc: 1.0000 - val_loss: 5.5234e-06 - val_acc: 1.0000\n",
      "Epoch 93/100\n",
      "21964/21964 [==============================] - 1s 52us/step - loss: 0.0011 - acc: 0.9998 - val_loss: 0.0015 - val_acc: 0.9993\n",
      "Epoch 94/100\n",
      "21964/21964 [==============================] - 1s 52us/step - loss: 2.5924e-06 - acc: 1.0000 - val_loss: 4.2800e-06 - val_acc: 1.0000\n",
      "Epoch 95/100\n",
      "21964/21964 [==============================] - 1s 52us/step - loss: 4.4482e-06 - acc: 1.0000 - val_loss: 3.1763e-07 - val_acc: 1.0000\n",
      "Epoch 96/100\n",
      "21964/21964 [==============================] - 1s 51us/step - loss: 0.0026 - acc: 0.9995 - val_loss: 0.0012 - val_acc: 0.9998\n",
      "Epoch 97/100\n",
      "21964/21964 [==============================] - 1s 51us/step - loss: 2.0194e-04 - acc: 0.9999 - val_loss: 4.3239e-06 - val_acc: 1.0000\n",
      "Epoch 98/100\n",
      "21964/21964 [==============================] - 1s 51us/step - loss: 4.5560e-07 - acc: 1.0000 - val_loss: 2.4606e-06 - val_acc: 1.0000\n",
      "Epoch 99/100\n",
      "21964/21964 [==============================] - 1s 52us/step - loss: 3.0507e-04 - acc: 0.9999 - val_loss: 2.5198e-04 - val_acc: 0.9998\n",
      "Epoch 100/100\n",
      "21964/21964 [==============================] - 1s 52us/step - loss: 0.0011 - acc: 0.9997 - val_loss: 0.0012 - val_acc: 0.9996\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2af887efa58>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils.np_utils import to_categorical\n",
    "#Con la data pre-procesada una capa\n",
    "#rprop is equivalent to using the gradient but also dividing by the size of the gradient.\n",
    "\"\"\"\n",
    "– The problem with mini-batch rprop is that we divide by a different number \n",
    "for each mini-batch. So why not force the number we divide by to be very \n",
    "similar for adjacent mini-batches?\n",
    "rmsprop: Keep a moving average of the squared gradient for each weight\"\"\"\n",
    "model8 = Sequential()\n",
    "model8.add(Dense(30, input_dim=Sx_tr.shape[1], init='uniform', activation='relu'))\n",
    "model8.add(Dense(30, init='uniform', activation='elu'))\n",
    "model8.add(Dense(25, init='uniform', activation='softmax'))\n",
    "model8.compile(optimizer=RMSprop(lr=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model8.fit(Sx_tr, to_categorical(y_tr), nb_epoch=100, batch_size=128, verbose=1,\n",
    "                validation_data=(Sx_v,to_categorical(y_v)))\n",
    "\n",
    "###NOS QUEDAMOS CON ESTA, logró val_acc 1 de manera consistente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7172/7172 [==============================] - 0s 34us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[3.3425941509836159, 0.72141662018962638]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "from sklearn.metrics import accuracy_score\n",
    "predicciones = model8.predict(Sx_t)\n",
    "accuracyTest = accuracy_score(predicciones,y_t)\n",
    "erroT = 1 - accuracyTest\n",
    "print(\"La mejor red encontrada fue el modelo 8.\")\n",
    "print(\"Error de Pruebas de la mejor red encontrada: \"+str(errorT))\n",
    "\"\"\"\n",
    "#predicciones = model8.predict(Sx_t)\n",
    "model8.evaluate(Sx_t, to_categorical(y_t), batch_size=128, verbose=1, sample_weight=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>(d) Para la mejor red entrenada anteriormente construya la matriz de confusión de las distintas clases, para\n",
    "asi visualizar cuáles son las clases más difíciles de clasificar y con cuáles se confunden. Comente.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[331,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [  0, 409,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   3,   0,  20,   0,   0],\n",
       "       [  0,   0, 268,   0,   0,  21,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,  21,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0, 197,   0,   0,   0,   0,   0,   0,   0,   0,  14,\n",
       "          0,   0,   0,   8,   0,   0,   2,   0,   0,  24,   0],\n",
       "       [  0,  19,   0,   0, 442,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,  37,   0,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0, 223,   0,   0,   0,   0,   1,   0,   0,\n",
       "          0,   0,   0,   0,   0,   3,   0,   0,  20,   0,   0],\n",
       "       [  0,   0,   3,   0,   0,   0, 230,  41,   0,   0,   0,   3,   0,\n",
       "          0,   0,  58,   0,   0,  13,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,  40, 349,   0,   0,   0,  21,   6,\n",
       "          0,   0,   0,   0,   0,  20,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,  21,   0,   0,   0,   0, 187,   0,   0,   0,  40,\n",
       "          0,   0,   0,   0,  19,  12,   0,   0,   0,   2,   7],\n",
       "       [  0,   0,   0,   0,   0,  42,   0,   0,  20, 152,   0,   0,   0,\n",
       "          0,   0,   0,  60,  21,   0,  16,   4,  16,   0,   0],\n",
       "       [  0,   0,  42,   0,   0,   2,   0,   0,   0,   0, 144,   0,   0,\n",
       "          0,   0,   0,   0,   0,  21,   0,   0,   0,   0,   0],\n",
       "       [ 18,  11,   0,   0,  23,   0,   0,   0,   0,   0,   0, 172,  44,\n",
       "         21,   0,  20,   0,  85,   0,   0,   0,   0,   0,   0],\n",
       "       [ 63,   0,  20,   0,   4,   0,   0,   0,   0,   0,   0,   8, 153,\n",
       "         22,   0,  21,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,  21,   0,   0,   0,   0,   0,   0,  16,\n",
       "        184,   0,  20,   0,   0,   5,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,  19,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          2, 305,  21,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   1,   0,\n",
       "          0,   0, 163,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,  21,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,  88,   0,   0,  35,   0,   0,   0,   0],\n",
       "       [  8,   0,   0,   0,  21,   0,   0,   0,   0,   0,   0,  41,   4,\n",
       "         17,   0,   0,   0, 155,   0,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,  20,   0,  21,   0,   1,   0,   0,\n",
       "          0,   0,   0,   0,   0, 183,   0,   0,   0,  23,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,  33,  22,   0,   0,\n",
       "          0,   0,   0,  57,   0,   0, 134,  20,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,  41,  20,   0,   0,  19,   0,   0,   0,\n",
       "          0,  18,   0,   2,   0,   0,   0, 197,  35,  14,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   4,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,  40,   5, 140,  17,   0],\n",
       "       [  0,   0,   0,   0,   0,  21,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,  43,   0,  20,   0,   0,   0, 183,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,  45,   0,  41,   0,   0,\n",
       "          0,   2,   0,  15,  18,  21,   0,   0,   5,   0, 185]], dtype=int64)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred = model8.predict(Sx_t)\n",
    "pred =[]\n",
    "for fila in y_pred:\n",
    "    pred.append(np.argmax(fila))\n",
    "confusion_matrix(y_t, pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "(e) Entrene una SVM no lineal sobre los pixeles con y sin pre-procesamiento. Puede utilizar el conjunto de\n",
    "validación para seleccionar hiper-parámetros, como el nivel de regularización aplicado y/o la función\n",
    "de kernel a utilizar.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Esto se demora entre 20-30 min en correr, hay que sintonizar los parametros usando x_v y Sx_v con y_v\n",
    "from sklearn.svm import SVC\n",
    "clf = SVC()\n",
    "clf.fit(x_tr, y_tr) \n",
    "clf2 = SVC()\n",
    "clf2.fit(Sx_tr, y_tr) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score Validacion sin Pre-Procesamiento: 0.226552540521\n",
      "Score Validacion Pre-Procesamiento: 0.99981788381\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "predTest1 = clf.predict(x_v)\n",
    "predTest2 = clf2.predict(Sx_v)\n",
    "acc1 = accuracy_score(predTest1,y_v)\n",
    "acc2 = accuracy_score(predTest2,y_v)\n",
    "print(\"Score Validacion sin Pre-Procesamiento: \"+str(acc1))\n",
    "print(\"Score Validacion Pre-Procesamiento: \"+str(acc2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score sin Pre-Procesamiento: 0.0200780814278\n",
      "Score con Pre-Procesamiento: 0.842861126603\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "predTest1 = clf.predict(x_t)\n",
    "predTest2 = clf2.predict(Sx_t)\n",
    "acc1 = accuracy_score(predTest1,y_t)\n",
    "acc2 = accuracy_score(predTest2,y_t)\n",
    "print(\"Score sin Pre-Procesamiento: \"+str(acc1))\n",
    "print(\"Score con Pre-Procesamiento: \"+str(acc2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "(f) Entrene una árbol de clasificación sobre los pixeles con y sin pre-procesamiento. Puede utilizar el\n",
    "conjunto de validación para seleccionar hiper-parámetros, como la profundidad máxima del árbol.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier as DTC\n",
    "arbol = DTC()\n",
    "arbol.fit(x_tr,y_tr)\n",
    "\n",
    "arbol2 = DTC()\n",
    "arbol2.fit(Sx_tr,y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score Validacion de DTC Sin Pre-Proc: 0.876160990712\n",
      "Score Validacion de DTC Con Pre-Proc: 0.874157712621\n"
     ]
    }
   ],
   "source": [
    "predT1 = arbol.predict(x_v)\n",
    "predT2 = arbol2.predict(Sx_v)\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "errorT1 = accuracy_score(predT1,y_v)\n",
    "print(\"Score Validacion de DTC Sin Pre-Proc: \"+str(errorT1))\n",
    "\n",
    "errorT2 = accuracy_score(predT2,y_v)\n",
    "print(\"Score Validacion de DTC Con Pre-Proc: \"+str(errorT2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score de DTC Sin Pre-Proc: 0.445203569437\n",
      "Score de DTC Con Pre-Proc: 0.443809258226\n"
     ]
    }
   ],
   "source": [
    "predT1 = arbol.predict(x_t)\n",
    "predT2 = arbol2.predict(Sx_t)\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "errorT1 = accuracy_score(predT1,y_t)\n",
    "print(\"Score de DTC Sin Pre-Proc: \"+str(errorT1))\n",
    "\n",
    "errorT2 = accuracy_score(predT2,y_t)\n",
    "print(\"Score de DTC Con Pre-Proc: \"+str(errorT2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
